mydata_HK2 <- join(mydata_HK, sat_output, by = "ring")
kable(head(mydata_HK2))
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  0.305   0.545   0.646   0.656   0.771   0.999
# sat <- read.csv(file = "D:/OneDrive - nyu.edu/NYU Marron/180716_Draw Circle/180720_Ring Saturation100.csv")
# https://drive.google.com/open?id=1DNyQLX0apRVWmRC1xKhpqZLE6SLcQF5p
id <- "1DNyQLX0apRVWmRC1xKhpqZLE6SLcQF5p" # google file ID
sat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
sat$idx <- rep(c(1:100),10)
# serve_site()
build_site()
getwd()
library(blogdown)
# serve_site()
build_site()
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# Column Water Vapor correction - Modified script for correcting AOD measurements
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(xgboost)
library(googledrive)
library(data.table)
options(digits = 6)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
# Column Water Vapor correction - Modified script for correcting AOD measurements
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(googledrive)
library(data.table)
library(xgboost)
options(digits = 6)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
## functions for plot
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xgb_model, shap_approx = TRUE,
X_train = cwv_data$train_mm)
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
# Column Water Vapor correction - Modified script for correcting AOD measurements
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(googledrive)
library(data.table)
library(xgboost)
options(digits = 6)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xgb_model, shap_approx = TRUE,
X_train = cwv_data$train_mm)
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = data_train)
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = X_train)
print(shap_result$mean_shap_score)
shap_long <- shap.prep(X_train = cwv_data$train_mm, top_n = dim(cwv_data$train_mm)[2])
shap_long <- shap.prep(X_train = X_train, top_n = dim(X_train)[2])
# make summary plot
plot.shap.summary(data_long = shap_long)
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
## functions for plot
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
require('ggplot2')
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = X_train)
print(shap_result$mean_shap_score)
shap_long <- shap.prep(X_train = X_train, top_n = dim(X_train)[2])
# make summary plot
plot.shap.summary(data_long = shap_long)
# make summary plot
plot.shap.summary(data_long = shap_long)
install.packages('ggforce')
# make summary plot
plot.shap.summary(data_long = shap_long)
shap.score.rank
shap_long <- shap.prep(X_train = X_train, top_n = 10)
# make summary plot
plot.shap.summary(data_long = shap_long)
f_ranked <- names(shap_result$mean_shap_score)
xgb.plot.shap(data = cwv_data$train_mm, model = xgb_model, features = f_ranked[1:9], n_col = 3)
xgb.plot.shap(data = X_train, model = xg_mod, features = f_ranked[1:9], n_col = 3)
print(shap_result$mean_shap_score)
print(shap_result$mean_shap_score)[1:10]
# serve_site()
build_site()
# serve_site()
build_site()
library(blogdown)
# serve_site()
build_site()
# serve_site()
build_site()
serve_site()
servr::daemon_stop("564274664")
build_site()
