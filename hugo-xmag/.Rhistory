data_test <- data.frame(X2, Y2)[-train_idx,]
autoxgbparset
head(data_train)
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
install.packages('rgenoud')
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
data_train
head(data_train)
head(mydata)
mydata
head(mydata)
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
head(mydata)
str(data_train)
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,][,-1]
head(data_train)
head(X2)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2$V1 <- NULL
head(X2)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2
head(X2)
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
head(X2)
# https://drive.google.com/open?id=1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X2 <- X2[,-1]
head(X2)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
# https://drive.google.com/open?id=1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2 <- X2[,-1]
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
head(data_train)
tunegrid <- structure(list(nrounds = 228, max_depth = 8, eta = 0.034, gamma = 0,
colsample_bytree = 0.7208, min_child_weight = 7, subsample = 0.7017),
Names = c("nrounds",
"max_depth", "eta", "gamma", "colsample_bytree", "min_child_weight",
"subsample"), row.names = 1L, class = "data.frame")
tunegrid
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
reg_task
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 3L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
reg_auto
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 80L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
reg_auto
saveRDS(reg_auto)
saveRDS(reg_auto, file = "D:/reg_auto_80.rds")
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((testdt$diffcwv - Y_test)^2))
sqrt(mean((txgb_pred - Y_test)^2))
sqrt(mean((xgb_pred - Y_test)^2))
sqrt(mean((xgb_pred - data_test$Share_Temporary)^2))
xgb_pred <- predict(reg_auto, data_test)
xgb_pred
xgb_pred$predict.type
sqrt(mean((xgb_pred$predict.type - data_test$Share_Temporary)^2))
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2))
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_80 <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"))
library(googledrive)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_80 <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"))
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_80 <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"))
options(httr_oauth_cache = 2)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_80 <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), overwrite = T)
print(reg_auto_80)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_file <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), overwrite = T)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_file <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), overwrite = T)
reg_auto_80 <- readRDS(reg_auto_file)
str(reg_auto_file)
reg_auto_file
reg_auto_80 <- readRDS(reg_auto_file)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
reg_auto_file <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), overwrite = T, "read80.rds")
reg_auto_file
getwd()
reg_auto_80 <- readRDS(tempdir("read80.rds"))
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
temp <- tempfile(fileext = ".rds")
reg_auto_file <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T, "read80.rds")
reg_auto_80 <- readRDS(tempdir("read80.rds"))
tempdir("read80.rds")
tempfile("read80.rds")
reg_auto_80 <- readRDS(tempfile("read80.rds"))
reg_auto_80 <- readRDS(paste(tempdir(), "read80.rds", sep = ""))
reg_auto_80 <- readRDS(paste(tempdir(), "\\read80.rds", sep = ""))
reg_auto_file <- drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
temp
reg_auto_80 <- readRDS(temp)
print(reg_auto_80)
drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
reg_auto_80 <- readRDS(temp)
print(reg_auto_80)
xgb_pred <- predict(reg_auto_80, data_test)
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2))
xgb_pred$data$response
length(xgb_pred$data$response )
dim(data_test)
best_rmse_index <- 228
best_rmse <- 0.2143
best_seednumber <- 7187
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 8,
eta = 0.034,   # Learning rate, default: 0.3
subsample = 0.7017,
colsample_bytree = 0.7208,
min_child_weight = 7, # These two are important
max_delta_step = 5)
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = dtest, params = best_param, nround = nround, verbose = F)
# MSE
yhat_xg <- predict(xg_mod, dtest)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = dtrain, params = best_param, nround = nround, verbose = F)
# MSE
yhat_xg <- predict(xg_mod, dtest)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
xgb_pred <- predict(reg_auto_80, data_test)
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2))
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
# 07/09/2018
list.of.packages <- c("ggplot2", "data.table","plyr","QuantPsyc",
"glmnet","leaps","randomForest","gbm","caret","xgboost","Ckmeans.1d.dp",
"DiagrammeR", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(data.table)    #
library(plyr)
library(ggplot2)
library(QuantPsyc)     # used to show standardized regression coefficients
library(glmnet)        # for Lasso
library(leaps)         # for best subset
library(randomForest)  # random forest
library(gbm)           # for Gradient boosting
library(caret)         # grid scan by train
library(xgboost)       # for Extreme Gradient boosting
library(Ckmeans.1d.dp) # for xgb.ggplot.importance in xgboost
library(DiagrammeR)    # for xgb.plot.tree in xgboost
library(knitr)
options(digits = 4)
options(tinytex.verbose = TRUE)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
mydata2 <- data.frame(X2, Y2)
# Matrix for xgb: dtrain
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
dim(data_train)
data_test <- data.frame(X2, Y2)[-train_idx,]
dim(data_test)
head(data_train)
# https://drive.google.com/open?id=1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2 <- X2[,-1]
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,];dim(data_test)
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]; dim(data_train)
head(data_train)
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
# system.time(reg_auto <- autoxgboost(reg_task))
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
reg_auto
xgb_pred <- predict(reg_auto_80, data_test)
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2))
reg_auto
# system.time(reg_auto <- autoxgboost(reg_task))
par1 <- autoxgbparset
par1
par1$pars
par1$pars$eta
par1$pars$eta$lower
par1$pars$max_depth$lower
par1$pars$max_depth$upper
par1 <- autoxgbparset
par1$pars$max_depth$lower <- 6
par1$pars$max_depth$upper <- 8
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 80L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl, par.set = par1))
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
reg_auto
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2))
sqrt(mean((xgb_pred$data$response - mydata[,"Share_Temporary"][-train_idx,])^2))
mydata[,"Share_Temporary"][-train_idx,]
# https://drive.google.com/open?id=1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2 <- X2[,-1]
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]; dim(data_train)
data_test <- data.frame(X2, Y2)[-train_idx,];dim(data_test)
library(googledrive)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
temp <- tempfile(fileext = ".rds")
drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
reg_auto <- readRDS(temp)
print(reg_auto)
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - mydata[,"Share_Temporary"][-train_idx,])^2))
xgb_pred$error
performance(yhat_xg, Y_test)
dim(mydata[,"Share_Temporary"])
mydata[,"Share_Temporary"]
X0 <- as.data.frame(X2)
head(mydata)
head(X_train)
head(mydata)
head(X_train)
# 07/09/2018
list.of.packages <- c("ggplot2", "data.table","plyr","QuantPsyc",
"glmnet","leaps","randomForest","gbm","caret","xgboost","Ckmeans.1d.dp",
"DiagrammeR", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(data.table)    #
library(plyr)
library(ggplot2)
library(QuantPsyc)     # used to show standardized regression coefficients
library(glmnet)        # for Lasso
library(leaps)         # for best subset
library(randomForest)  # random forest
library(gbm)           # for Gradient boosting
library(caret)         # grid scan by train
library(xgboost)       # for Extreme Gradient boosting
library(Ckmeans.1d.dp) # for xgb.ggplot.importance in xgboost
library(DiagrammeR)    # for xgb.plot.tree in xgboost
library(knitr)
options(digits = 4)
options(tinytex.verbose = TRUE)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
mydata2 <- data.frame(X2, Y2)
# Matrix for xgb: dtrain
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
head(data_train)
head(data_test)
head(mydata)
head(data_train)
xgb_pred <- predict(reg_auto, data_test)
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
# system.time(reg_auto <- autoxgboost(reg_task))
par1 <- autoxgbparset
par1$pars$max_depth$lower <- 6
par1$pars$max_depth$upper <- 8
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 80L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl, par.set = par1))
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - Y_test)))
reg_auto
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
# system.time(reg_auto <- autoxgboost(reg_task))
# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
reg_auto
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - Y_test)))
library(googledrive)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
temp <- tempfile(fileext = ".rds")
drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
reg_auto <- readRDS(temp)
print(reg_auto)
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - Y_test)))
serve_site()
build_site()
gs_user()
library(googledrive)
gs_user()
