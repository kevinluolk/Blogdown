Cough = c(0,1,0,1),
y = c(0,0,0,80)
))
X1 = as.matrix(d[,.(Fever, Cough)])
m1 = xgboost::xgboost(
data = X1, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0, nrounds = 1, verbose = F)
shap_m <- shap.values(m1, X1)
shap_long_m <- shap.prep(shap_m, X1)
names(shap_m$shap_score) <- paste0("SHAP.", names(shap_m$shap_score))
output_simple <- cbind(x = X1, y.actual = d$y, y.pred = predict(m1, X1), shap_m$shap_score, BIAS = shap_m$BIAS0)
?plot.shap.dependence
?plot.shap.force_plot
# show that rowSum is the output
shap_data <- copy(shap_values_mod$shap_score)
shap_data[, BIAS := shap_values_mod$BIAS0]
shap_data[, `:=`(rowSum = round(rowSums(shap_data),6), pred_mod = round(pred_mod,6))]
rmarkdown::paged_table(shap_data[1:30,])
knitr::kable(d)
X1 = as.matrix(d[,.(Fever, Cough)])
X2 = as.matrix(d[,.(Cough, Fever)])
m1 = xgboost(
data = X1, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",  verbose = F)
m2 = xgboost(
data = X2, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",verbose = F)
xgb.importance(model = m1)
xgb.importance(model = m2)
library(xgboost)
X1 = as.matrix(d[,.(Fever, Cough)])
X2 = as.matrix(d[,.(Cough, Fever)])
m1 = xgboost(
data = X1, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",  verbose = F)
m2 = xgboost(
data = X2, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",verbose = F)
xgb.importance(model = m1)
xgb.importance(model = m2)
knitr::kable(output_simple)
force_plot_data <- shap.stack.data(shap_values_mod$shap_score, n_groups = 6)
plot.shap.force_plot(force_plot_data)
plot.shap.force_plot_bygroup(force_plot_data)
plot.summary.plot <- function(shap_long2, sat_name){
# levels_origin <- levels(shap_long2$variable)
summary_labels_list <- list(dayint = 'Time trend',
Column_WV = 'MAIAC CWV',
AOT_Uncertainty = 'Blue band\nuncertainty',
aod = "AOD",
elev = "Elevation",
dist_water_km = "Distance to\nwater",
DevAll_P1km = "% Developed",
forestProp_1km = "% Forest",
RelAZ = "Relative\nazimuth angle"
)
levels(shap_long2$variable) <- unlist(summary_labels_list[levels(shap_long2$variable)])
# make the summary plot:
p <-  plot.shap.summary(shap_long2, x_bound_given = 0.9)
return(p + ggtitle(sat_name))
}
g1 <- plot.shap.summary(shap_long_mod, "Terra (Model)")
g2 <- plot.shap.summary(shap_long, "Terra (CV)")
library(gridExtra)
grid.arrange(g1,g2, ncol=2)
plot.summary.plot <- function(shap_long2, sat_name){
# levels_origin <- levels(shap_long2$variable)
summary_labels_list <- list(dayint = 'Time trend',
Column_WV = 'MAIAC CWV',
AOT_Uncertainty = 'Blue band\nuncertainty',
aod = "AOD",
elev = "Elevation",
dist_water_km = "Distance to\nwater",
DevAll_P1km = "% Developed",
forestProp_1km = "% Forest",
RelAZ = "Relative\nazimuth angle"
)
levels(shap_long2$variable) <- unlist(summary_labels_list[levels(shap_long2$variable)])
# make the summary plot:
p <-  plot.shap.summary(shap_long2, x_bound_given = 0.9)
return(p)
}
g1 <- plot.shap.summary(shap_long_mod, "Terra (Model)")
plot.summary.plot <- function(shap_long2, sat_name){
# levels_origin <- levels(shap_long2$variable)
summary_labels_list <- list(dayint = 'Time trend',
Column_WV = 'MAIAC CWV',
AOT_Uncertainty = 'Blue band\nuncertainty',
aod = "AOD",
elev = "Elevation",
dist_water_km = "Distance to\nwater",
DevAll_P1km = "% Developed",
forestProp_1km = "% Forest",
RelAZ = "Relative\nazimuth angle"
)
levels(shap_long2$variable) <- unlist(summary_labels_list[levels(shap_long2$variable)])
# make the summary plot:
p <-  plot.shap.summary(shap_long2, x_bound_given = 0.9)
return(p)
}
g1 <- plot.summary.plot(shap_long_mod, "Terra (Model)")
g2 <- plot.summary.plot(shap_long, "Terra (CV)")
library(gridExtra)
grid.arrange(g1,g2, ncol=2)
g3 <- plot.shap.dependence.color(data_long = shap_long_mod, x = 'dayint', y_shap = 'dayint', color_feature = 'Column_WV') + ggtitle("SHAP of Date vs. Feature values of Date")
g3
# interaction feature j: Column_WV
#
# plot.shap.dependence.color(data_long = shap_long_mod, x = i_feature, y_shap = i_feature, color_feature = j_feature)
plot.shap.interact <- function(i_feature, j_feature, plot_main = F){
xgb_mod2 <- rfe.fit(X = as.matrix(dataX[,-..j_feature]), Y = as.matrix(dataXY_df[[y_var]]), xgb_param = param_dart)
shap_values_mod2 <- shap.score.rank(xgb_model = xgb_mod2, X_train = dataX[,-..j_feature])
shap_long_mod2 <- shap.long.data.prep(shap_values_mod2, dataX[,-..j_feature])
# the original shap
data_i <- shap_long_mod[variable == i_feature,]   # fit w j
data_i2 <- shap_long_mod2[variable == i_feature,]  # fit w/o j
data_i$main_effect <- data_i2$value
data_i[, int_effect := value - main_effect]
data_i$color_value <- shap_long_mod[variable == j_feature, rfvalue]
# the SHAP range
i_feature_range <- range(data_i$value)
plot.shap.interact.core <- function(main){
if (main) y0 = "main_effect" else y0 = "int_effect"
if (main) ylab = paste0("SHAP main effect for ", label.feature(i_feature)) else ylab  = paste0("SHAP interaction value for\n", label.feature(i_feature), " and ", label.feature(j_feature))
plot1 <- ggplot(data = data_i,
aes_string(x = "rfvalue", y = y0, color = "color_value"))+
geom_point(size = 0.2, alpha = 0.6)+
# a loess smoothing line:
geom_smooth(method = 'loess', color = 'red', size = 0.4, se = F) +
labs(y = ylab,
x = label.feature(i_feature),
color = paste0(label.feature(j_feature),"\n","(Feature value)")) +
scale_color_gradient(low="#FFCC33", high="#6600CC",
guide = guide_colorbar(barwidth = 10, barheight = 0.3)) +
# scale_y_continuous(limits = i_feature_range) +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_text(size=10),
legend.text=element_text(size=8)) +
ggtitle(if(main)"Main Effect"else"Interaction Effect")
plot1
}
plot1 <- plot.shap.interact.core(main = T)
plot2 <- plot.shap.interact.core(main = F)
return(if(plot_main)  grid.arrange(plot1, plot2, ncol=2) else plot2)
}
int1 = plot.shap.interact(i_feature = "dayint", j_feature = "Column_WV")
int2 = plot.shap.interact(i_feature = "Column_WV", j_feature = "AOT_Uncertainty")
grid.arrange(grobs = list(int1, int2), ncol = 2)
# fig_list <- lapply(var_list_a, plot.shap.interact, i_feature = "Column_WV",
#                    plot_main  = F)
# fig_grid_a <- grid.arrange(grobs = fig_list, ncol = 2)
# ggsave(fig_grid_a, file = here("Figure", paste0(date0,"_Interaction_by_Feature_terra.png")), width = 6, height = 10)
plot.shap.interact <- function(i_feature, j_feature, plot_main = F){
xgb_mod2 <- xgboost.fit(X = as.matrix(dataX[,-..j_feature]), Y = as.matrix(dataXY_df[[y_var]]), xgb_param = param_dart)
shap_values_mod2 <- shap.score.rank(xgb_model = xgb_mod2, X_train = dataX[,-..j_feature])
shap_long_mod2 <- shap.long.data.prep(shap_values_mod2, dataX[,-..j_feature])
# the original shap
data_i <- shap_long_mod[variable == i_feature,]   # fit w j
data_i2 <- shap_long_mod2[variable == i_feature,]  # fit w/o j
data_i$main_effect <- data_i2$value
data_i[, int_effect := value - main_effect]
data_i$color_value <- shap_long_mod[variable == j_feature, rfvalue]
# the SHAP range
i_feature_range <- range(data_i$value)
plot.shap.interact.core <- function(main){
if (main) y0 = "main_effect" else y0 = "int_effect"
if (main) ylab = paste0("SHAP main effect for ", label.feature(i_feature)) else ylab  = paste0("SHAP interaction value for\n", label.feature(i_feature), " and ", label.feature(j_feature))
plot1 <- ggplot(data = data_i,
aes_string(x = "rfvalue", y = y0, color = "color_value"))+
geom_point(size = 0.2, alpha = 0.6)+
# a loess smoothing line:
geom_smooth(method = 'loess', color = 'red', size = 0.4, se = F) +
labs(y = ylab,
x = label.feature(i_feature),
color = paste0(label.feature(j_feature),"\n","(Feature value)")) +
scale_color_gradient(low="#FFCC33", high="#6600CC",
guide = guide_colorbar(barwidth = 10, barheight = 0.3)) +
# scale_y_continuous(limits = i_feature_range) +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_text(size=10),
legend.text=element_text(size=8)) +
ggtitle(if(main)"Main Effect"else"Interaction Effect")
plot1
}
plot1 <- plot.shap.interact.core(main = T)
plot2 <- plot.shap.interact.core(main = F)
return(if(plot_main)  grid.arrange(plot1, plot2, ncol=2) else plot2)
}
int1 = plot.shap.interact(i_feature = "dayint", j_feature = "Column_WV")
int2 = plot.shap.interact(i_feature = "Column_WV", j_feature = "AOT_Uncertainty")
grid.arrange(grobs = list(int1, int2), ncol = 2)
shap_long_mod2 <- shap.prep(shap_values_mod2, dataX[,-..j_feature])
plot.shap.interact <- function(i_feature, j_feature, plot_main = F){
xgb_mod2 <- xgboost.fit(X = as.matrix(dataX[,-..j_feature]), Y = as.matrix(dataXY_df[[y_var]]), xgb_param = param_dart)
shap_values_mod2 <- shap.values(xgb_model = xgb_mod2, X_train = dataX[,-..j_feature])
shap_long_mod2 <- shap.prep(shap_values_mod2, dataX[,-..j_feature])
# the original shap
data_i <- shap_long_mod[variable == i_feature,]   # fit w j
data_i2 <- shap_long_mod2[variable == i_feature,]  # fit w/o j
data_i$main_effect <- data_i2$value
data_i[, int_effect := value - main_effect]
data_i$color_value <- shap_long_mod[variable == j_feature, rfvalue]
# the SHAP range
i_feature_range <- range(data_i$value)
plot.shap.interact.core <- function(main){
if (main) y0 = "main_effect" else y0 = "int_effect"
if (main) ylab = paste0("SHAP main effect for ", label.feature(i_feature)) else ylab  = paste0("SHAP interaction value for\n", label.feature(i_feature), " and ", label.feature(j_feature))
plot1 <- ggplot(data = data_i,
aes_string(x = "rfvalue", y = y0, color = "color_value"))+
geom_point(size = 0.2, alpha = 0.6)+
# a loess smoothing line:
geom_smooth(method = 'loess', color = 'red', size = 0.4, se = F) +
labs(y = ylab,
x = label.feature(i_feature),
color = paste0(label.feature(j_feature),"\n","(Feature value)")) +
scale_color_gradient(low="#FFCC33", high="#6600CC",
guide = guide_colorbar(barwidth = 10, barheight = 0.3)) +
# scale_y_continuous(limits = i_feature_range) +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_text(size=10),
legend.text=element_text(size=8)) +
ggtitle(if(main)"Main Effect"else"Interaction Effect")
plot1
}
plot1 <- plot.shap.interact.core(main = T)
plot2 <- plot.shap.interact.core(main = F)
return(if(plot_main)  grid.arrange(plot1, plot2, ncol=2) else plot2)
}
int1 = plot.shap.interact(i_feature = "dayint", j_feature = "Column_WV")
int1
# Example use iris
library(SHAPforxgboost)
X1 = as.matrix(iris[,-5])
mod1 = xgboost::xgboost(
data = X1, label = iris$Species, gamma = 0, eta = 1, lambda = 0,nrounds = 1, verbose = F)
# shap.values() has the SHAP data matrix and ranked features by mean|SHAP|
shap_values <- shap.values(mod1, X1)
# ranked features:
shap_values$mean_shap_score
# shap.prep() returns the long-format SHAP data
shap_long <- shap.prep(shap_values, X1)
# or
shap_long <- shap.prep(shap.values(mod1, X1), X1)
# ****
plot.shap.summary(shap_long)
# Alternatives:
# option 1: from the xgboost model
plot.shap.summary.wrap1(mod1, X1, top_n = 3)
# option 2: supply a self-made SHAP values dataset (e.g. sometimes as output from cross-validation)
plot.shap.summary.wrap2(shap_score = shap_values$shap_score, X1, top_n = 3)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
plot.shap.summary.wraps
plot.shap.summary.wrap1(xg_mod, X_train, 9)
dim(X_train)
f_ranked <- names(shap_result$mean_shap_score)[1:6]
mod1 <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
plot.shap.summary.wrap1(mod1, X_train, top_n = 10)
shap_values <- shap.values(xgb_model = mod1, X_train = X_train)
f_ranked <- names(shap_values$mean_shap_score)[1:6]
f_ranked
features_ranked <- names(shap_values$mean_shap_score)[1:6]
xgb.plot.shap(data = X_train, model = xg_mod, features = f_ranked[1:6], n_col = 3)
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, n_groups = 4)
plot.shap.force_plot(force_plot_data)
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, top_n  = 6, n_groups = 4)
plot.shap.force_plot(force_plot_data)
plot.shap.force_plot_bygroup(force_plot_data)
devtools::install_github("liuyanguu/SHAPforxgboost")
library(blogdown)
library(here)
here()
build_site()
devtools::install_github("liuyanguu/SHAPforxgboost")
source(here::here("Code","prepare_data_model.R"))
shap_data[, `:=`(rowSum = round(rowSums(shap_data),6), pred_mod = round(pred_mod,6))]
# show that rowSum is the output
shap_data <- copy(shap_values_mod$shap_score)
shap_data[, BIAS := shap_values_mod$BIAS0]
shap_data[, `:=`(rowSum = round(rowSums(shap_data),6), pred_mod = round(pred_mod,6))]
rmarkdown::paged_table(shap_data[1:30,])
# interaction feature j: Column_WV
#
# plot.shap.dependence.color(data_long = shap_long_mod, x = i_feature, y_shap = i_feature, color_feature = j_feature)
plot.shap.interact <- function(i_feature, j_feature, plot_main = F){
xgb_mod2 <- xgboost.fit(X = as.matrix(dataX[,-..j_feature]), Y = as.matrix(dataXY_df[[y_var]]), xgb_param = param_dart)
shap_values_mod2 <- shap.values(xgb_model = xgb_mod2, X_train = dataX[,-..j_feature])
shap_long_mod2 <- shap.prep(shap_values_mod2, dataX[,-..j_feature])
# the original shap
data_i <- shap_long_mod[variable == i_feature,]   # fit w j
data_i2 <- shap_long_mod2[variable == i_feature,]  # fit w/o j
data_i$main_effect <- data_i2$value
data_i[, int_effect := value - main_effect]
data_i$color_value <- shap_long_mod[variable == j_feature, rfvalue]
# the SHAP range
i_feature_range <- range(data_i$value)
plot.shap.interact.core <- function(main){
if (main) y0 = "main_effect" else y0 = "int_effect"
if (main) ylab = paste0("SHAP main effect for ", label.feature(i_feature)) else ylab  = paste0("SHAP interaction value for\n", label.feature(i_feature), " and ", label.feature(j_feature))
plot1 <- ggplot(data = data_i,
aes_string(x = "rfvalue", y = y0, color = "color_value"))+
geom_point(size = 0.2, alpha = 0.6)+
# a loess smoothing line:
geom_smooth(method = 'loess', color = 'red', size = 0.4, se = F) +
labs(y = ylab,
x = label.feature(i_feature),
color = paste0(label.feature(j_feature),"\n","(Feature value)")) +
scale_color_gradient(low="#FFCC33", high="#6600CC",
guide = guide_colorbar(barwidth = 10, barheight = 0.3)) +
# scale_y_continuous(limits = i_feature_range) +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_text(size=10),
legend.text=element_text(size=8)) +
ggtitle(if(main)"Main Effect"else"Interaction Effect")
plot1
}
plot1 <- plot.shap.interact.core(main = T)
plot2 <- plot.shap.interact.core(main = F)
return(if(plot_main)  grid.arrange(plot1, plot2, ncol=2) else plot2)
}
int1 = plot.shap.interact(i_feature = "dayint", j_feature = "Column_WV")
int2 = plot.shap.interact(i_feature = "Column_WV", j_feature = "AOT_Uncertainty")
grid.arrange(grobs = list(int1, int2), ncol = 2)
# fig_list <- lapply(var_list_a, plot.shap.interact, i_feature = "Column_WV",
#                    plot_main  = F)
# fig_grid_a <- grid.arrange(grobs = fig_list, ncol = 2)
# ggsave(fig_grid_a, file = here("Figure", paste0(date0,"_Interaction_by_Feature_terra.png")), width = 6, height = 10)
devtools::install_github("liuyanguu/SHAPforxgboost")
devtools::install_github("liuyanguu/SHAPforxgboost")
library(SHAPforxgboost)
SHAPforxgboost::label.feature("AOD")
# Example use iris
library(SHAPforxgboost)
# Example use iris
library(SHAPforxgboost)
X1 = as.matrix(iris[,-5])
mod1 = xgboost::xgboost(
data = X1, label = iris$Species, gamma = 0, eta = 1, lambda = 0,nrounds = 1, verbose = F)
# shap.values() has the SHAP data matrix and ranked features by mean|SHAP|
shap_values <- shap.values(mod1, X1)
# ranked features:
shap_values$mean_shap_score
# shap.prep() returns the long-format SHAP data
shap_long <- shap.prep(shap_values, X1)
# or
shap_long <- shap.prep(shap.values(mod1, X1), X1)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
mod1 <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
plot.shap.summary.wrap1(mod1, X_train, top_n = 10)
shap_values <- shap.values(xgb_model = mod1, X_train = X_train)
features_ranked <- names(shap_values$mean_shap_score)[1:6]
plot.shap.dependence.color
data_long = shap.prep(shap_values, X_train, 6)
data_long
fir_list <- lapply(features_ranked, plot.shap.dependence, shap_long = shap_long)
fir_list <- lapply(features_ranked, plot.shap.dependence, data_long = shap_long)
fig_list <- lapply(features_ranked, plot.shap.dependence, data_long = shap_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
features_ranked
fig_list <- lapply(features_ranked, plot.shap.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
build_site()
library(blogdown)
library(here)
build_site()
xgboost::xgb.plot.shap(data = X_train, model = xg_mod, features = features_ranked, n_col = 3)
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 3)
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 3)
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 2)
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 2)
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, top_n  = 6, n_groups = 4)
plot.shap.force_plot(force_plot_data)
build_site()
suppressPackageStartupMessages({
library(xgboost)
library(DiagrammeR)
})
options(digits = 4)
set.seed(123)
n0 <- 5
X <-  data.frame(x1 = runif(n0), x2 = runif(n0))
Y <-  c(1, 5, 20, 50, 100)
cbind(X, Y)
# non-zero skip_drop has higher priority than rate_drop or one_drop
param_gbtree <- list(objective = 'reg:linear', nrounds = 3,
eta = 2,
max_depth = 10,
min_child_weight = 0,
booster = 'gbtree'
)
simple.xgb.output <- function(param,...){
set.seed(1234)
m = xgboost(data = as.matrix(X), label = Y, params = param,
nrounds = param$nround,
base_score = 0)
cat('Evaluation log showing testing error:\n')
print(m$evaluation_log)
pred <- predict(m, as.matrix(X), ntreelimit = param$nrounds)
cat('Predicted values of Y: \n')
print(pred)
pred2 <- predict(m, as.matrix(X), predcontrib = TRUE)
cat("SHAP value for X: \n")
print(pred2)
p <- xgb.plot.tree(model = m)
p
}
simple.xgb.output(param_gbtree)
