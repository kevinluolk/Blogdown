eta = 0.018,
max_depth = 10,
gamma = 0.009,
subsample = 0.98,
colsample_bytree = 0.86)
mod <- xgboost.fit(X = as.matrix(dataX), Y = as.matrix(dataXY_df[[y_var]]),
xgb_param = param_dart)
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod, X_train = dataX)
# The ranked features by mean |SHAP|
shap_values$mean_shap_score
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = mod, X_train = dataX)
shap.plot.summary(shap_long, x_bound  = 1.2, dilute = 10)
# without color version but has marginal distribution,
# just plot SHAP value against feature value:
fig_list <- lapply(names(shap_values$mean_shap_score)[1:4],
shap.plot.dependence, data_long = shap_long_mod)
g3 <- shap.plot.dependence.color(data_long = shap_long, x = 'dayint', y = 'dayint', color_feature = 'Column_WV') + ggtitle("SHAP of Date vs. Feature values of Date")
# option 1: from the xgboost model
shap.plot.summary.wrap1(mod, X = as.matrix(dataX))
here()
library(here)
# run the model with built-in data
library("SHAPforxgboost"); library("ggplot2"); library("xgboost"); library("here")
saveRDS(shap_long, here("Intermediate/190718_shap_long.rds"))
shap_long <- readRDS(here("Intermediate/190718_shap_long.rds"))
n
# prepare the data using either:
# (this step is slow since it calculates all the combinations of features.)
data_int <- shap.prep.interaction(xgb_mod = mod, X_train = as.matrix(dataX))
saveRDS(shap_int, here("Intermediate/190718_shap_int.rds"))
saveRDS(data_int, here("Intermediate/190718_shap_int.rds"))
shap_int <- readRDS(here("Intermediate/190718_shap_int.rds"))
# **SHAP interaction effect plot **
shap.plot.dependence.color(data_long = shap_long,
data_int = shap_int,
x= "Column_WV",
y = "AOT_Uncertainty",
color_feature = "AOT_Uncertainty")
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n = 4, n_groups = 6)
shap.plot.force_plot(plot_data)
pred_mod <- predict(mod)
pred_mod <- predict(mod, as.matrix(dataX))
# show that rowSum is the output
shap_data <- shap_values$shap_score
shap_data[, BIAS := shap_values$BIAS0]
pred_mod <- predict(mod, as.matrix(dataX))
shap_data[, `:=`(rowSum = round(rowSums(shap_data),6), pred_mod = round(pred_mod,6))]
rmarkdown::paged_table(shap_data[1:20,])
# run the model with built-in data
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
shap.plot.force_plot(plot_data)
# run the model with built-in data
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
# run the model with built-in data
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
library("data.table"); library("here")
# To get the interaction SHAP dataset for plotting:
# fit the xgboost model
mod1 = xgboost::xgboost(
data = as.matrix(iris[,-5]), label = iris$Species,
gamma = 0, eta = 1, lambda = 0,nrounds = 1, verbose = FALSE)
# Use either:
data_int <- shap.prep.interaction(xgb_mod = mod1,
X_train = as.matrix(iris[,-5]))
# or:
shap_int <- predict(mod1, as.matrix(iris[,-5]),
predinteraction = TRUE)
# **SHAP interaction effect plot **
shap.plot.dependence.color(data_long = shap_long_iris,
data_int = shap_int_iris,
x="Petal.Length",
y = "Petal.Width",
color_feature = "Petal.Width")
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
mod1 <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
shap.plot.summary.wrap1(mod1, X_train, top_n = 10)
shap_values <- shap.values(xgb_model = mod1, X_train = X_train)
data_long = shap.prep(shap_values, X_train, 6)
features_ranked <- names(shap_values$mean_shap_score)[1:6]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
data_long = shap.prep(mod1, X_train, 6)
data_long = shap.prep(mod1, X_train)
data_long = shap.prep(mod1, X_train = X_train)
data_long = shap.prep(mod1, X_train = X_train)
features_ranked <- names(shap_values$mean_shap_score)[1:6]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
xgboost::xgb.shap.plot(data = X_train, model = mod1, features = features_ranked, n_col = 2)
plot(data = X_train, model = mod1, features = features_ranked, n_col = 2)
library(SHAPforxgboost)
library(xgboost)
library(data.table)
library(ggplot2)
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, top_n  = 6, n_groups = 4)
force_plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n  = 6, n_groups = 4)
shap.plot.force_plot(force_plot_data)
shap.plot.force_plot_bygroup(force_plot_data)
xgboost::xgb.shap.plot
plot(data = X_train, model = mod1, features = features_ranked, n_col = 2)
xgboost::xgb.shap.plot(data = X_train, model = mod1, features = features_ranked, n_col = 2)
?xgb.plot.shap
xgb.shap.plot(data = X_train, model = mod1, features = features_ranked, n_col = 2)
xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 2)
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 2)
xgboost::xgb.plot.shap(data = X_train, model = mod1, top_n = 4, n_col = 2)
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
serve_site()
build_site()
# Example use iris
library(SHAPforxgboost)
library(xgboost)
library(data.table)
library(ggplot2)
X1 = as.matrix(iris[,-5])
mod1 = xgboost::xgboost(
data = X1, label = iris$Species, gamma = 0, eta = 1,
lambda = 0,nrounds = 1, verbose = F)
# shap.values(model, X_dataset) returns the SHAP
# data matrix and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod1, X_train = X1)
shap_values$mean_shap_score
shap_values_iris <- shap_values$shap_score
# shap.prep() returns the long-format SHAP data from either model or
shap_long_iris <- shap.prep(xgb_model = mod1, X_train = X1)
# is the same as: using given shap_contrib
shap_long_iris <- shap.prep(shap_contrib = shap_values_iris, X_train = X1)
# ****
shap.plot.summary(shap_long_iris)
shap.plot.summary(shap_long_iris, x_bound  = 1.5, dilute = 10)
# option 1: from the xgboost model
shap.plot.summary.wrap1(mod1, X1, top_n = 3)
# option 2: supply a self-made SHAP values dataset (e.g. sometimes as output from cross-validation)
shap.plot.summary.wrap2(shap_score = shap_values$shap_score, X1, top_n = 3)
# **SHAP dependence plot**
shap.plot.dependence.color(data_long = shap_long_iris, x="Petal.Length",
y = "Petal.Width", color_feature = "Petal.Width")
# the without color version, just plot SHAP value against feature value:
shap.plot.dependence(data_long = shap_long_iris, "Petal.Length")
# To get the interaction SHAP dataset for plotting:
# fit the xgboost model
mod1 = xgboost::xgboost(
data = as.matrix(iris[,-5]), label = iris$Species,
gamma = 0, eta = 1, lambda = 0,nrounds = 1, verbose = FALSE)
# Use either:
data_int <- shap.prep.interaction(xgb_mod = mod1,
X_train = as.matrix(iris[,-5]))
# or:
shap_int <- predict(mod1, as.matrix(iris[,-5]),
predinteraction = TRUE)
# **SHAP interaction effect plot **
shap.plot.dependence.color(data_long = shap_long_iris,
data_int = shap_int_iris,
x="Petal.Length",
y = "Petal.Width",
color_feature = "Petal.Width")
# **SHAP force plot**
plot_data <- shap.prep.stack.data(shap_contrib = shap_values_iris,
n_groups = 4)
shap.plot.force_plot(plot_data)
shap.plot.force_plot(plot_data,  zoom_in_group = 2)
# plot all the clusters:
shap.plot.force_plot_bygroup(plot_data)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
mydata
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
mod1 <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
shap.plot.summary.wrap1(mod1, X_train, top_n = 10)
data_long = shap.prep(mod1, X_train = X_train)
features_ranked <- names(shap_values$mean_shap_score)[1:6]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
features_ranked
data_long = shap.prep(mod1, X_train)
data_long = shap.prep(mod1, X_train = X_train)
features_ranked <- shap.values(mod1, X_train)
features_ranked
features_ranked <- shap.values(mod1, X_train)$mean_shap_score[1:4]
features_ranked
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
data_long = shap.prep(mod1, X_train = X_train)
features_ranked <- shap.values(mod1, X_train)$mean_shap_score[1:4]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
shap.plot.dependence
shap.plot.dependence(data_long = data_long, show_feature = features_ranked[1])
data_long
# the without color version, just plot SHAP value against feature value:
shap.plot.dependence(data_long = shap_long_iris, "Petal.Length")
features_ranked[1]
features_ranked <- names(shap.values(mod1, X_train)$mean_shap_score)[1:4]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
force_plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n  = 4, n_groups = 4)
shap.plot.force_plot(force_plot_data)
shap_values <- shap.values(mod1, X_train)
features_ranked <- names(shap_values$mean_shap_score)[1:4]
fig_list <- lapply(features_ranked, shap.plot.dependence, data_long = data_long)
force_plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n  = 4, n_groups = 4)
shap.plot.force_plot(force_plot_data)
shap.plot.force_plot_bygroup(force_plot_data)
build_site()
# **SHAP interaction effect plot **
g3 <- shap.plot.dependence.color(data_long = shap_long,
data_int = shap_int,
x= "dayint", y = "Column_WV",
color_feature = "Column_WV")
shap_long <- readRDS(here("Intermediate/190718_shap_long.rds"))
# **SHAP interaction effect plot **
g3 <- shap.plot.dependence.color(data_long = shap_long,
data_int = shap_int,
x= "dayint", y = "Column_WV",
color_feature = "Column_WV")
shap_int <- readRDS(here("Intermediate/190718_shap_int.rds"))
# **SHAP interaction effect plot **
g3 <- shap.plot.dependence.color(data_long = shap_long,
data_int = shap_int,
x= "dayint", y = "Column_WV",
color_feature = "Column_WV")
g4 <- shap.plot.dependence.color(data_long = shap_long,
data_int = shap_int,
x= "Column_WV", y = "AOT_Uncertainty",
color_feature = "AOT_Uncertainty")
gridExtra::grid.arrange(g3, g4, ncol=2)
build_site()
data_train
dim(data_train)
dim(shap_score)
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = mod, X_train = dataX)
# **SHAP summary plot**
shap.plot.summary(shap_long)
# diluted points:
shap.plot.summary(shap_long, x_bound  = 1.2, dilute = 10)
source('~/.active-rstudio-document', echo=TRUE)
devtools::install_github("liuyanguu/SHAPforxgboost")
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
devtools::install_github("liuyanguu/SHAPforxgboost")
# Example use iris
library(SHAPforxgboost)
library(xgboost)
library(data.table)
library(ggplot2)
X1 = as.matrix(iris[,-5])
mod1 = xgboost::xgboost(
data = X1, label = iris$Species, gamma = 0, eta = 1,
lambda = 0,nrounds = 1, verbose = F)
# shap.values(model, X_dataset) returns the SHAP
# data matrix and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod1, X_train = X1)
shap_values$mean_shap_score
shap_values_iris <- shap_values$shap_score
# shap.prep() returns the long-format SHAP data from either model or
shap_long_iris <- shap.prep(xgb_model = mod1, X_train = X1)
# is the same as: using given shap_contrib
shap_long_iris <- shap.prep(shap_contrib = shap_values_iris, X_train = X1)
shap.plot.summary(shap_long_iris)
# option of dilute is offered to make plot faster if there are over thousands of observations
# please see documentation for details.
shap.plot.summary(shap_long_iris, x_bound  = 1.5, dilute = 10)
```{r}
shap.plot.dependence.color(data_long = shap_long_iris, x="Petal.Length",
y = "Petal.Width", color_feature = "Petal.Width")
shap.plot.dependence.color(data_long = shap_long_iris, x="Petal.Length",
y = "Petal.Width", color_feature = "Petal.Width")
# the without color version, just plot SHAP value against feature value:
shap.plot.dependence(data_long = shap_long_iris, "Petal.Length")
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n = 4, n_groups = 6)
# run the model with built-in data
library("SHAPforxgboost"); library("ggplot2"); library("xgboost")
library("data.table"); library("here")
y_var <-  "diffcwv"
dataX <- dataXY_df[,-..y_var]
# hyperparameter tuning results
param_dart <- list(objective = "reg:linear",  # For regression
nrounds = 366,
eta = 0.018,
max_depth = 10,
gamma = 0.009,
subsample = 0.98,
colsample_bytree = 0.86)
mod <- xgboost.fit(X = as.matrix(dataX), Y = as.matrix(dataXY_df[[y_var]]),
xgb_param = param_dart)
# To return the SHAP values and ranked features by mean|SHAP|
shap_values <- shap.values(xgb_model = mod, X_train = dataX)
# The ranked features by mean |SHAP|
shap_values$mean_shap_score
# is the same as: using given shap_contrib
shap_long <- shap.prep(shap_contrib = shap_values$shap_score, X_train = dataX)
# choose to show top 4 features by setting `top_n = 4`, set 6 clustering groups.
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n = 4, n_groups = 6)
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_zoomin_limit = c(-1,1))
shap.plot.force_plot
# choose to zoom in at location 500, set zoom in y-axis limit using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_zoomin_limit = c(-1,1), y_parent_limit = c(-1,1))
# choose to zoom in at location 500, set zoom in y-axis limit using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))
# The ranked features by mean |SHAP|
shap_values$mean_shap_score
# show that rowSum is the output
shap_data <- shap_values$shap_score
shap_data[, BIAS := shap_values$BIAS0]
pred_mod <- predict(mod, as.matrix(dataX))
shap_data[, `:=`(rowSum = round(rowSums(shap_data),6), pred_mod = round(pred_mod,6))]
rmarkdown::paged_table(shap_data[1:20,])
library(xgboost)
X1 = as.matrix(d[,.(Fever, Cough)])
X2 = as.matrix(d[,.(Cough, Fever)])
m1 = xgboost(
data = X1, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",  verbose = F)
m2 = xgboost(
data = X2, label = d$y,base_score = 0, gamma = 0, eta = 1, lambda = 0,nrounds = 1,objective = "reg:linear",verbose = F)
xgb.importance(model = m1)
xgb.importance(model = m2)
# choose to show top 4 features by setting `top_n = 4`, set 6 clustering groups.
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, top_n = 4, n_groups = 6)
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1,1))
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-0.5,1))
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-0.5,1.5))
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_parent_limit = c(-1.5,1.5))
# choose to zoom in at location 500, set y-axis limit using `y_parent_limit`
# it is also possible to set y-axis limit for zoom-in part alone using `y_zoomin_limit`
shap.plot.force_plot(plot_data, zoom_in_location = 500, y_zoomin_limit = c(-0.5,1.5))
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
blogdown:::new_post_addin()
source('~/.active-rstudio-document', echo=TRUE)
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
# some quick commend to use
library(blogdown)
library(here)
here()
# blogdown::new_site(theme = 'alanorth/hugo-theme-bootstrap4-blog')
# setwd(here("hugo-xmag"))
# serve_site()
build_site()
