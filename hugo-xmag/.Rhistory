library(caret)         # grid scan by train
library(xgboost)       # for Extreme Gradient boosting
library(Ckmeans.1d.dp) # for xgb.ggplot.importance in xgboost
library(DiagrammeR)    # for xgb.plot.tree in xgboost
library(knitr)
options(digits = 4)
options(tinytex.verbose = TRUE)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
mydata2 <- data.frame(X2, Y2)
# Matrix for xgb: dtrain
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = dtrain, params = best_param, nround = nround, verbose = F)
# MSE
yhat_xg <- predict(xg_mod, dtest)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
xg_mod <- xgboost(data = dtrain, params = best_param, nround = nround, verbose = T)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
# Matrix for xgb: dtrain
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
# MSE
yhat_xg <- predict(xg_mod, dtest)
xg_mod <- xgboost(data = dtrain, params = best_param, nround = nround, verbose = T)
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = dtrain, params = best_param, nround = nround, verbose = T)
# MSE
yhat_xg <- predict(xg_mod, dtest)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
head(dtrain)
head(X_train)
data_train <- data.frame(X_train, Y_train)
head(data_train)
data_train <- as.data.frame(X_train, Share_Temporary = Y_train)
head(data_train)
data_train <- cbind(X_train, Share_Temporary = Y_train)
head(data_train)
data_train <- cbind(X_train, Share_Temporary = Y_train)
data_test <- cbind(X_test, Share_Temporary = Y
data_train <- cbind(X_train, Share_Temporary = Y_train)
data_train <- cbind(X_train, Share_Temporary = Y_train)
data_test <- cbind(X_test, Share_Temporary = Y_test)
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
data_train <- data.frame(cbind(X_train, Share_Temporary = Y_train) )
str(data_train)
data_train <- data.frame(cbind(X_train, Share_Temporary = Y_train) )
data_test <- data.frame(cbind(X_test, Share_Temporary = Y_test)  )
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
# If need to change iterations or control, all are very easy:
# MBOctrl <- makeMBOControl()
# ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
# system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
# saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - Y_test)))
reg_auto
Y_test
Y_train
Y2
head(Y_test)
head(Y2)
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
head(data_train)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
# MSE
yhat_xg <- predict(xg_mod, data = X_test, label = Y_test)
# MSE
yhat_xg <- predict(xg_mod, data = X_test)
# MSE
yhat_xg <- predict(xg_mod, newdata = X_test)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
pred.autoxgb(reg_auto, test_X = X_test)
pred.autoxgb <- function(model0, test_X){
xgb_pred <- predict(model0, test_X)
rmse0 <- sqrt(mean((xgb_pred$data$truth - xgb_pred$data$response)^2))
cat('rmse is: ', rmse0, "\n")
}
pred.autoxgb(reg_auto, test_X = X_test)
# MSE
yhat_xg <- predict(xg_mod, newdata = data_test)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
# MSE
yhat_xg <- predict(xg_mod, newdata = X_test)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
# MSE
yhat_xg <- predict(xg_mod, newdata = X_test)
(MSE_xgb <- mean((yhat_xg - Y_test)^2))
head(data_test)
pred.autoxgb(reg_auto, test_X = data_test)
pred.autoxgb
reg_auto
# library(googledrive)
# # https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
# temp <- tempfile(fileext = ".rds")
# drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
# reg_auto <- readRDS(temp)
reg_auto <- readRDS('D:/liuyanguu/Blogdown/SDIautoxgboost_80.rds')
print(reg_auto)
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - Y_test)))
sqrt(mean((xgb_pred$data$response - xgb_pred$data$truth)))
# 07/09/2018
list.of.packages <- c("ggplot2", "data.table","plyr","QuantPsyc",
"glmnet","leaps","randomForest","gbm","caret","xgboost","Ckmeans.1d.dp",
"DiagrammeR", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(data.table)    #
library(plyr)
library(ggplot2)
library(QuantPsyc)     # used to show standardized regression coefficients
library(glmnet)        # for Lasso
library(leaps)         # for best subset
library(randomForest)  # random forest
library(gbm)           # for Gradient boosting
library(caret)         # grid scan by train
library(xgboost)       # for Extreme Gradient boosting
library(Ckmeans.1d.dp) # for xgb.ggplot.importance in xgboost
library(DiagrammeR)    # for xgb.plot.tree in xgboost
library(knitr)
options(digits = 4)
options(tinytex.verbose = TRUE)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
mydata2 <- data.frame(X2, Y2)
# Matrix for xgb: dtrain
dtrain <- xgb.DMatrix(X_train, label = Y_train)
dtest <- xgb.DMatrix(X_test, label = Y_test)
dim(X_train)
dim(Y_train)
head(Y_train)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
head(Y_train)
tunegrid <- structure(list(nrounds = 228, max_depth = 8, eta = 0.034, gamma = 0,
colsample_bytree = 0.7208, min_child_weight = 7, subsample = 0.7017),
Names = c("nrounds",
"max_depth", "eta", "gamma", "colsample_bytree", "min_child_weight",
"subsample"), row.names = 1L, class = "data.frame")
tunegrid
# library(googledrive)
# # https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
# temp <- tempfile(fileext = ".rds")
# drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
# reg_auto <- readRDS(temp)
reg_auto <- readRDS('D:/liuyanguu/Blogdown/SDIautoxgboost_80.rds')
print(reg_auto)
pred.autoxgb <- function(model0, test_X){
xgb_pred <- predict(model0, test_X)
rmse0 <- sqrt(mean((xgb_pred$data$truth - xgb_pred$data$response)^2))
cat('rmse is: ', rmse0, "\n")
}
pred.autoxgb(reg_auto, test_X = data_test)
xgb_pred <- predict(reg_auto, data_test)
sqrt(mean((xgb_pred$data$response - xgb_pred$data$truth)))
xgb_pred <- predict(reg_auto, data_test)
mean((xgb_pred$data$response - xgb_pred$data$truth))
xgb_pred <- predict(reg_auto, data_test)
mean((xgb_pred$data$response - xgb_pred$data$truth)^2)
autoxgbparset.mixed = makeParamSet(
makeDiscreteParam("booster", values = c("gbtree", "gblinear", "dart")),
makeDiscreteParam("sample_type", values = c("uniform", "weighted"), requires = quote(booster == "dart")),
makeDiscreteParam("normalize_type", values = c("tree", "forest"), requires = quote(booster == "dart")),
makeNumericParam("rate_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
makeNumericParam("skip_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
makeLogicalParam("one_drop", requires = quote(booster == "dart")),
makeDiscreteParam("grow_policy", values = c("depthwise", "lossguide")),
makeIntegerParam("max_leaves", lower = 0, upper = 8, trafo = function(x) 2^x, requires = quote(grow_policy == "lossguide")),
makeIntegerParam("max_bin", lower = 2L, upper = 9, trafo = function(x) 2^x),
makeNumericParam("eta", lower = 0.01, upper = 0.2),
makeNumericParam("gamma", lower = -7, upper = 6, trafo = function(x) 2^x),
makeIntegerParam("max_depth", lower = 3, upper = 20),
makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
makeNumericParam("colsample_bylevel", lower = 0.5, upper = 1),
makeNumericParam("lambda", lower = -10, upper = 10, trafo = function(x) 2^x),
makeNumericParam("alpha", lower = -10, upper = 10, trafo = function(x) 2^x),
makeNumericParam("subsample", lower = 0.5, upper = 1)
)
system.time(reg_auto_dart <- autoxgboost(reg_task, par.set = autoxgbparset.mixed))
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
autoxgbparset.mixed = makeParamSet(
makeDiscreteParam("booster", values = c("gbtree", "gblinear", "dart")),
makeDiscreteParam("sample_type", values = c("uniform", "weighted"), requires = quote(booster == "dart")),
makeDiscreteParam("normalize_type", values = c("tree", "forest"), requires = quote(booster == "dart")),
makeNumericParam("rate_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
makeNumericParam("skip_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
makeLogicalParam("one_drop", requires = quote(booster == "dart")),
makeDiscreteParam("grow_policy", values = c("depthwise", "lossguide")),
makeIntegerParam("max_leaves", lower = 0, upper = 8, trafo = function(x) 2^x, requires = quote(grow_policy == "lossguide")),
makeIntegerParam("max_bin", lower = 2L, upper = 9, trafo = function(x) 2^x),
makeNumericParam("eta", lower = 0.01, upper = 0.2),
makeNumericParam("gamma", lower = -7, upper = 6, trafo = function(x) 2^x),
makeIntegerParam("max_depth", lower = 3, upper = 20),
makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
makeNumericParam("colsample_bylevel", lower = 0.5, upper = 1),
makeNumericParam("lambda", lower = -10, upper = 10, trafo = function(x) 2^x),
makeNumericParam("alpha", lower = -10, upper = 10, trafo = function(x) 2^x),
makeNumericParam("subsample", lower = 0.5, upper = 1)
)
system.time(reg_auto_dart <- autoxgboost(reg_task, par.set = autoxgbparset.mixed))
xgb_pred <- predict(reg_auto_dart, data_test)
mean((xgb_pred$data$response - Y_test)^2)
reg_auto
reg_auto_dart
saveRDS(reg_auto_dart, file = "D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
saveRDS(reg_auto_dart, file = "D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
# saveRDS(reg_auto_dart, file = "D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
reg_auto_dart <- readRDS("D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
print(reg_auto_dart)
xgb_pred <- predict(reg_auto_dart, data_test)
mean((xgb_pred$data$response - Y_test)^2)
# saveRDS(reg_auto_dart, file = "D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
reg_auto_dart <- readRDS("D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
print(reg_auto_dart)
xgb_pred <- predict(reg_auto_dart, data_test)
cat("testing mse: \n")
mean((xgb_pred$data$response - Y_test)^2)
getwd()
library(blogdown)
# serve_site()
build_site()
knitr::opts_chunk$set(echo = T, warning = FALSE, message = FALSE)
# Where are all the data files located:
parent_dir <- c("D:/OneDrive - nyu.edu/NYU Marron/180716_Draw Circle/Data/")
# Get all the folder names (names of the cities) in the directory, used to scan all the cities
city_list <- list.files(parent_dir)
# If use Hong Kong as an example
city1 <- city_list[6]
newdir <- paste(parent_dir, city1, sep = "")
image <- raster(paste(newdir, c("./city_urbFootprint_clp_t3.img"), sep = ""))
list.of.packages <- c("raster", "rgdal", "Hmisc", "plyr", "RColorBrewer", "googledrive")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library('raster')   # obtain raster from GIS image file, and plot
library('rgdal')    # readOGR: read ArcGIS vector maps into Spatial objects
library('Hmisc')    # cut2: cut vector into equal-length
library('plyr')     # join
library('RColorBrewer') # for `brewer.pal` in ggplot2
library('knitr')    # kable
options(digits = 4)
# Where are all the data files located:
parent_dir <- c("D:/OneDrive - nyu.edu/NYU Marron/180716_Draw Circle/Data/")
# Get all the folder names (names of the cities) in the directory, used to scan all the cities
city_list <- list.files(parent_dir)
# If use Hong Kong as an example
city1 <- city_list[6]
newdir <- paste(parent_dir, city1, sep = "")
image <- raster(paste(newdir, c("./city_urbFootprint_clp_t3.img"), sep = ""))
# A point that shows the center of the city
cbd <-  readOGR(dsn = newdir, layer = paste(city1, "_CBD_Project", sep=""))
cbdpoint <- SpatialPoints(cbd)
# mydata_HK contains coordinate (x,y) and category (type)
mydata_HK <- as.data.frame(rasterToPoints(image))
names(mydata_HK) <- c("x", "y", "type")
# calculate distance to the cbd from every point
pts <- as.matrix(mydata_HK[,1:2])
mydata_HK$cbd_dist <- spDistsN1(pts, cbdpoint, longlat = F)
# library('Hmisc') # cut2
mydata_HK$ring <- as.numeric(cut2(mydata_HK$cbd_dist,g = 100)) # get 1:100
mydata_HK$type <- as.factor(mydata_HK$type)
mydata_HK$ring <- as.factor(mydata_HK$ring)
# Function to get saturation, as the raster have 7 layers, 1 to 3 belong to built-up area
get_sat <- function(x){
x <- as.factor(x)
# (1 + 2 + 3) / (1 + 2 + 3 + 4 + 5 + 6 + 7)
sum(x==1|x==2|x==3)/length(x)
}
# get saturation by rings
sat_output <- aggregate(mydata_HK$type, by = list(mydata_HK$ring), FUN = get_sat)
names(sat_output) <- c("ring", "ring_saturation")
# join back to mydata_HK2 so we can later plot by values of ring_saturation
mydata_HK2 <- join(mydata_HK, sat_output, by = "ring")
kable(head(mydata_HK2))
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#  0.305   0.545   0.646   0.656   0.771   0.999
# sat <- read.csv(file = "D:/OneDrive - nyu.edu/NYU Marron/180716_Draw Circle/180720_Ring Saturation100.csv")
# https://drive.google.com/open?id=1DNyQLX0apRVWmRC1xKhpqZLE6SLcQF5p
id <- "1DNyQLX0apRVWmRC1xKhpqZLE6SLcQF5p" # google file ID
sat <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
sat$idx <- rep(c(1:100),10)
# serve_site()
build_site()
