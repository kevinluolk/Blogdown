install.packages('blogdown')
library(blogdown)
setwd("D:/OneDrive/R_Blog_Github/Blogdown/hugo-xmag")
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
list.of.packages <- c("ggplot2", "readxl","plyr","QuantPsyc",
"glmnet","leaps","randomForest","gbm","caret","xgboost","Ckmeans.1d.dp",
"DiagrammeR", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
library(readxl)
library(plyr)
library(ggplot2)
library(QuantPsyc)     # used to show standardized regression coefficients
library(glmnet)        # for Lasso
library(leaps)         # for best subset
library(randomForest)  # random forest
library(gbm)           # for Gradient boosting
library(caret)         # grid scan by train
library(xgboost)       # for Extreme Gradient boosting
library(Ckmeans.1d.dp) # for xgb.ggplot.importance in xgboost
library(DiagrammeR)    # for xgb.plot.tree in xgboost
library(knitr)
options(digits = 4)
options(tinytex.verbose = TRUE)
# Data Cleaning
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE,
fig.width=12, fig.height=12)
```{r}
library(curl)       # read file from google drive
install.packages('curl')
install.packages('gplots')
install.packages('dendextend')
install.packages('colorspace')
install.packages("colorspace")
knitr::opts_chunk$set(echo = T, warning = FALSE, message = FALSE)
list.of.packages <- c("raster", "rgdal","Hmisc","plyr", "RColorBrewer")
if(length(new.packages)) install.packages(new.packages)
library('foreign')
library('equivalence')
library('lubridate')
install.packages('equivalence')
install.packages('lubridate')
install.packages("lubridate")
rm(list=ls())
library('foreign')
library('equivalence')
library('lubridate')
install.packages('lubridate')
library('lubridate')
rm(list=ls())
library('foreign')
library('equivalence')
library('lubridate')
# functions  ----------------------------------------------------------
## get mode
Get_Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
# Sensitivity: producers accuracy built up
# Get sensitivity: 11/ (11 + 01)
get_Sen <- function(var){
sum(var=='11')/(sum(var=='11') + sum(var=='01'))
}
# Specificty: producers accuracy open space
# Get specifity: 00 / (00 + 10)
get_Spe <- function(var){
sum(var=='00')/(sum(var=='00') + sum(var=='10'))
}
# PPV Positive predictive value: users accuracy bulit up
# Get PPV: 11 / (11 + 01)
get_PPV <- function(var){
sum(var=='11')/(sum(var=='11') + sum(var=='10'))
}
# NPV Negative predictive value: users accuracy open space
# Get NPV: 00 / (00 + 01)
get_NPV <- function(var){
sum(var=='00')/(sum(var=='00') + sum(var=='01'))
}
# Overall accuracy = (11 + 00) / (11 + 00 + 10 + 01)
get_Accu <- function(var){
(sum(var=='00') + sum(var=='11'))/length(var)
}
# Get percent of Builtup tiles in each locale (ID_string)
get_Builtup <- function(var) {sum(var)/length(var)}
## Combine functions above, get confusion matrix
group_Analysis <- function(Var_GT){
Var <- substr(substitute(Var_GT),1,4)
GT_Sen <- get_Sen(Var_GT)
GT_Spe <- get_Spe(Var_GT)
GT_PPV <- get_PPV(Var_GT)
GT_NPV <- get_NPV(Var_GT)
GT_Acc <- get_Accu(Var_GT)
out <- data.frame(GT_Sen, GT_Spe, GT_PPV, GT_NPV, GT_Acc)
names(out) <- c(paste(Var,"vsGT_Sensitivity", sep = ""), paste(Var,"vsGT_Specificty", sep = ""),
paste(Var,"vsGT_PPV", sep = ""), paste(Var,"vsGT_NPV", sep = ""),
paste(Var,"vsGT_Accuracy", sep = ""))
return(out)
}
getAllLocales <- function(dir){
File_Directory <- paste(dir, "/Compare_point_identity.dbf", sep = "")
city <- read.dbf(File_Directory)
# clean the variable names
names(city) <- gsub("_", "", x = names(city))
require('lubridate')
city$Imgdate <- ymd(city$Imgdate)
G1 <- aggregate(x = city$IDstring, by = list(city$IDstring),
FUN = length)
names(G1) <- c("IDstring","Num_Pixel")
## name of the group & the city
G1$City <- levels(city$City)[1]
G1$IDstring <- paste("_", G1$IDstring, sep = "")
## dates
G1$GT_Date <- aggregate(x = city$Imgdate, by = list(city$IDstring), FUN = mean)$x
# average Builtup in all locales of a city
G1$NYU_Builtup <- aggregate(x = city$NYU, by = list(city$IDstring),
FUN = get_Builtup)$x
G1$GHS_Builtup <- aggregate(x = city$GHSL, by = list(city$IDstring),
FUN = get_Builtup)$x
G1$GT_Builtup <- aggregate(x = city$GT, by = list(city$IDstring),
FUN = get_Builtup)$x
## Binary builtup, turns builtup in each locale into binary judgement
G1$Binary_GT <- G1$Binary_GHS <- G1$Binary_NYU <- rep(0, length(G1$GT_Builtup))
G1$Binary_NYU[G1$NYU_Builtup>=0.5] <- 1
G1$Binary_GHS[G1$GHS_Builtup>=0.5] <- 1
G1$Binary_GT[G1$GT_Builtup>=0.5] <- 1
## is there any big locale with pixel > 400?
G1$Big_Locale <- 0
G1$Big_Locale[G1$Num_Pixel > 400] <- 1
G1 <- G1[G1$Big_Locale == 0, ]  # remove big locales
G1 <- subset(G1, select = -Big_Locale) # remove this column
return(G1)
}
#>Merge all the locales ------------------------------------------------------
parent_dir <- "D:/OneDrive - nyu.edu/Accuracy/"
dir_list <- list.dirs(parent_dir)[-1]
G_test <- getAllLocales(dir_list[37])
head(G_test)
df <- getAllLocales(dir_list[1])
write.table(df, "D:/OneDrive - nyu.edu/NYU Marron/Data3_R/0721_Accuracy All Locales.csv",
col.names = T, row.names = F, append = F, sep = ",")
attachResult <- function(x){
# print(x)
write.table(getAllLocales(x),"D:/OneDrive - nyu.edu/NYU Marron/Data3_R/0721_Accuracy All Locales.csv",
col.names = F, row.names = F, append = T, sep = ",")
}
Blank <- lapply(dir_list[-1],attachResult)
#>Analysis of the <all-locale> output ------------------------------------------------
##
AccLocales <- read.csv("D:/OneDrive - nyu.edu/NYU Marron/Data3_R/0721_Accuracy All Locales.csv")
head(AccLocales)
Binary_NYU_GT  <- paste(AccLocales$Binary_NYU, AccLocales$Binary_GT, sep="")
Binary_GHS_GT  <- paste(AccLocales$Binary_GHS, AccLocales$Binary_GT, sep="")
Binary_NYU_GHS <- paste(AccLocales$Binary_NYU, AccLocales$Binary_GHS, sep="")
group_Analysis_2 <- function(Var_GT){
GT_Sen <- get_Sen(Var_GT)
GT_Spe <- get_Spe(Var_GT)
GT_PPV <- get_PPV(Var_GT)
GT_NPV <- get_NPV(Var_GT)
GT_Acc <- get_Accu(Var_GT)
out <- data.frame(GT_Sen, GT_Spe, GT_PPV, GT_NPV, GT_Acc)
return(out)
}
G_test <- getAllLocales(dir_list[37])
head(G_test)
rm(list=ls())
library('foreign')
library('equivalence')
library('lubridate')
## Functions  ----------------------------------------------------------
# get mode
Get_Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
# Sensitivity: producers accuracy built up
# Get sensitivity: 11/ (11 + 01)
get_Sen <- function(var){
sum(var=='11')/(sum(var=='11') + sum(var=='01'))
}
# Specificty: producers accuracy open space
# Get specifity: 00 / (00 + 10)
get_Spe <- function(var){
sum(var=='00')/(sum(var=='00') + sum(var=='10'))
}
# PPV Positive predictive value: users accuracy bulit up
# Get PPV: 11 / (11 + 01)
get_PPV <- function(var){
sum(var=='11')/(sum(var=='11') + sum(var=='10'))
}
# NPV Negative predictive value: users accuracy open space
# Get NPV: 00 / (00 + 01)
get_NPV <- function(var){
sum(var=='00')/(sum(var=='00') + sum(var=='01'))
}
# Overall accuracy = (11 + 00) / (11 + 00 + 10 + 01)
get_Accu <- function(var){
(sum(var=='00') + sum(var=='11'))/length(var)
}
# Get percent of Builtup tiles in each locale (ID_string)
get_Builtup <- function(var) {sum(var)/length(var)}
## Combine functions above, get confusion matrix
group_Analysis <- function(Var_GT){
Var <- substr(substitute(Var_GT),1,4)
GT_Sen <- get_Sen(Var_GT)
GT_Spe <- get_Spe(Var_GT)
GT_PPV <- get_PPV(Var_GT)
GT_NPV <- get_NPV(Var_GT)
GT_Acc <- get_Accu(Var_GT)
out <- data.frame(GT_Sen, GT_Spe, GT_PPV, GT_NPV, GT_Acc)
names(out) <- c(paste(Var,"vsGT_Sensitivity", sep = ""), paste(Var,"vsGT_Specificty", sep = ""),
paste(Var,"vsGT_PPV", sep = ""), paste(Var,"vsGT_NPV", sep = ""),
paste(Var,"vsGT_Accuracy", sep = ""))
return(out)
}
# group_Analysis(NYU_GT)
## Get output by locale
group_Analysis_locale <- function(data, var){
Var_GT <- paste(data[[var]], data$GT, sep="")
G1 <- aggregate(x = Var_GT, by = list(data$IDstring),
FUN = length)
names(G1) <- c("IDstring","Num_Pixel")
# average Builtup in all locales of a city
G1$Builtup <- aggregate(x = data[[var]], by = list(data$IDstring),
FUN = get_Builtup)$x
G1$GT_Builtup <- aggregate(x = data$GT, by = list(data$IDstring),
FUN = get_Builtup)$x
# the sum of the number of pixels
G1$Builtup_Pixels <- aggregate(x = data[[var]], by = list(data$IDstring), FUN = sum)$x
G1$GT_Builtup_Pixels <- aggregate(x = data$GT, by = list(data$IDstring),FUN = sum)$x
## Binary builtup, turns builtup in each locale into binary judgement
G1$Builtup_bi <- G1$GT_Builtup_bi <-rep(0, length(G1$Builtup))
G1$Builtup_bi[G1$Builtup>=0.5] <- 1
G1$GT_Builtup_bi[G1$GT_Builtup>=0.5] <- 1
## name of the group for t-test output
G1$Var <- substr(var, 1, 3)
## is there any big locale with pixel > 400?
G1$Big_Locale <- 0
G1$Big_Locale[G1$Num_Pixel > 400] <- 1
G1$Big_Count <- 0; G1$Big_Count[1] <- sum(G1$Big_Locale == 1)
G1 <- G1[G1$Big_Locale == 0, ] # remove the big locales
return(G1)
}
# G0 <- group_Analysis_locale(city,"NYU")
# head(G0)
## get t-test and TOST of <tested group vs. GT>
group_t_test <- function(G1, ep = 0.1){
require('equivalence')
# assume 10% difference is ok.
Var <- G1$Var[1] # which group is it: NYU/GHS.
# 2-sample t-test -> revised to paired t-test
t_test <- t.test(x = G1$GT_Builtup, y = G1$Builtup, paired = T)
Mean_Diff <- -1 * t_test$estimate # Diff in paired t-test is x - y
Mean_Abs_Diff <- abs(Mean_Diff)
t_pValue <- t_test$p.value
# # tost
# tost_pValue <- tost(x = G1$GT_Builtup, y = G1$Builtup, epsilon = ep)$tost.p.value
# # Yes: means there is no difference between two means.
# I incl. two ways of binary results
No_Diff_1 <- ifelse(t_pValue>0.05, 1, 0)
No_Diff_2 <- ifelse(t_pValue>0.05, 1, -1)
out <- data.frame(t_pValue, No_Diff_1, No_Diff_2,
Mean_Diff, Mean_Abs_Diff)
# output
names(out) <- c(paste(Var,"vsGT_t_pValue", sep = ""),
paste(Var,"vsGT_NoDiff1", sep = ""),
paste(Var,"vsGT_NoDiff2", sep = ""),
paste(Var,"vsGT_MeanDiff", sep = ""),
paste(Var,"vsGT_AbsMeanDiff", sep = "")
)
return(out)
}
# Get Output  ------------------------------------------------------
getOutput <- function(dir){
File_Directory <- paste(dir, "/Compare_point_identity.dbf", sep = "")
city <- read.dbf(File_Directory)
# clean the variable names
# Use gsub ("group substitution") to align the variables names
names(city) <- gsub("_", "", x = names(city))
# city name
City <- levels(city$City)[1]
# dimensions
N_pixels <- dim(city)[1]
N_locals <- length(unique(city$IDstring))
Skipped <- mean(city$Skipped, na.rm = T)
# date
require("lubridate")
Img_level <- levels(city$Imgdate)
Img_length <- length(levels(city$Imgdate));
Img_date1 <- Img_level[1]
Img_date3 <- Img_level[Img_length]
# comparing with a baseline date to get variation
city$Imgdate <- ymd(city$Imgdate)
Img_date_sd <- sd(city$Imgdate, na.rm = T)
Img_date_mean <- mean(city$Imgdate, na.rm = T)
## The whole city All the locale together
# analysis for var = NYU__Name_ or GHSL__Name
# bind var with GT
NYU1_GT <- paste(city$NYU, city$GT, sep="")
GHS1_GT <- paste(city$GHSL, city$GT, sep="")
NvG <-     paste(city$NYU, city$GHSL, sep="")
# use group_Analysis to get the confusion matrix output
NYU_Matrix <- group_Analysis(NYU1_GT)
GHS_Matrix <- group_Analysis(GHS1_GT)
NvG_Matrix <- group_Analysis(NvG)
# by locale --------------------------------------------------
G1 <- group_Analysis_locale(data = city, var = "NYU")
Big_Locale <- G1$Big_Count[1]
G2 <- group_Analysis_locale(data = city, var = "GHSL")
# mean
GT_Builtup_mean   <- mean(G1$GT_Builtup, na.rm = T)
NYU_Builtup_mean  <- mean(G1$Builtup, na.rm = T)
GHSL_Builtup_mean <- mean(G2$Builtup, na.rm = T)
## binary output on locale level ##
# paste two binary to get sensitivity
NYU2_GT_locale <- paste(G1$Builtup_bi, G1$GT_Builtup_bi, sep="")
GHS2_GT_locale <- paste(G2$Builtup_bi, G2$GT_Builtup_bi, sep="")
NvG_locale <- paste(G1$Builtup_bi, G2$Builtup_bi, sep="")
NYU_Matrix_locale <- group_Analysis(NYU2_GT_locale)
GHS_Matrix_locale <- group_Analysis(GHS2_GT_locale)
NvG_Matrix_locale <- group_Analysis(NvG_locale)
# equalvance test --------------------------------------------------------
t1 <- group_t_test(G1)
t2 <- group_t_test(G2)
NvG_test <- t.test(x = G1$Builtup, y = G2$Builtup, paired = T)
NvG_Mean_Diff <- NvG_test$estimate # Diff in paired t-test is x - y, so NYU - GHSL
NvG_Mean_Abs_Diff <- abs(NvG_Mean_Diff)
NvG_t_pValue <- NvG_test$p.value
All_Output <- data.frame(
# Descriptive
City, N_pixels, N_locals, Skipped,
Img_date1, Img_date_mean, Img_date3, Img_date_sd,
# On Pixels level: (NYU1, GHS1)
NYU_Matrix, GHS_Matrix,
# On Locale level (NYU2, GHS2):
Big_Locale, GT_Builtup_mean,
NYU_Builtup_mean, t1, NYU_Matrix_locale,
GHSL_Builtup_mean, t2, GHS_Matrix_locale,
# NYU vs GHSL
NvG_Matrix, NvG_t_pValue,NvG_Mean_Diff,NvG_Mean_Abs_Diff, NvG_Matrix_locale,
row.names ="")
return(All_Output)
}
parent_dir <- "D:/OneDrive - nyu.edu/Accuracy/"
dir_list <- list.dirs(parent_dir)
getOutput(dir_list[20])
# install.packages('raster')
library('raster')   # obtain raster from GIS image file, and plot
library('rgdal')    # readOGR: read ArcGIS vector maps into Spatial objects
library('Hmisc')    # cut2
library('plyr')     # join
library('RColorBrewer') # for `brewer.pal` in ggplot2
library('knitr')    # kable
options(digits = 4)
# get saturation
get_sat <- function(x){
x <- as.factor(x)
# (1 + 2 + 3) / (1 + 2 + 3 + 4 + 5 + 6 + 7)
sum(x==1|x==2|x==3)/length(x)
}
parent_dir <- c("D:/OneDrive - nyu.edu/NYU Marron/180716_Draw Circle/Data/")
# Get all the folder names (names of the cities) in the directory
city_list <- list.files(parent_dir)
sat_out <- data.frame()
par(mar = c(1,2,1,0), bty="n") # c(bottom, left, top, right)
layout(matrix(c(1:10),ncol=2, byrow = T))
# length(city_list)
for (i in 1:10){
city1 <- city_list[i]
newdir <- paste(parent_dir, city1, sep = "")
image <- raster(paste(newdir, c("./city_urbFootprint_clp_t3.img"), sep = ""))
# mydata: df that contains coordinate and category
mydata <- as.data.frame(rasterToPoints(image))
names(mydata) <- c("x", "y", "type")
pts <- as.matrix(mydata[,1:2])
# cbd
cbd <-  readOGR(dsn = newdir, layer = paste(city1, "_CBD_Project", sep=""))
cbdpoint <- SpatialPoints(cbd)
# distance to the cbd
mydata$cbd_dist <- spDistsN1(pts, cbdpoint, longlat = F)
# library('Hmisc') # cut2
mydata$ring <- as.numeric(cut2(mydata$cbd_dist,g = 100))
mydata$type <- as.factor(mydata$type)
mydata$ring <- as.factor(mydata$ring)
sat_output <- aggregate(mydata$type, by = list(mydata$ring), FUN = get_sat)
sat_output$City <- city1
sat_output$City_Saturation <- get_sat(mydata$type)
names(sat_output) <- c("ring", "ring_saturation","city", "city_saturation")
sat_out <- rbind(sat_out, sat_output)
# draw the figure ---------------------------------------------------------
mydata2 <- join(mydata, sat_output, by = "ring")
r1 <- rasterFromXYZ(mydata2)
# writeRaster(r1, paste(raster_dir, city1, sep = ""), format = "HFA", overwrite=TRUE)
# To preview the color panel:
# display.brewer.pal(n = 12, name = 'RdYlGn')
color_scale <- brewer.pal(n = 10, name = "RdYlGn")
myPalette <- colorRampPalette(rev(color_scale))
plot(r1$ring_saturation, col=myPalette(70), breaks = c(30:100)/100,
legend=FALSE,  main = city1)
plot(cbdpoint, add = TRUE, size = 3) # drawn on top of a map using `add = TRUE`
}
# install.packages('raster')
library('raster')   # obtain raster from GIS image file, and plot
library('raster')   # obtain raster from GIS image file, and plot
list.of.packages <- c("raster", "rgdal","Hmisc","plyr", "RColorBrewer")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# install.packages('raster')
library('raster')   # obtain raster from GIS image file, and plot
library('rgdal')    # readOGR: read ArcGIS vector maps into Spatial objects
library('Hmisc')    # cut2
library('plyr')     # join
library('RColorBrewer') # for `brewer.pal` in ggplot2
library('knitr')    # kable
options(digits = 4)
# length(city_list)
for (i in 1:10){
city1 <- city_list[i]
newdir <- paste(parent_dir, city1, sep = "")
image <- raster(paste(newdir, c("./city_urbFootprint_clp_t3.img"), sep = ""))
# mydata: df that contains coordinate and category
mydata <- as.data.frame(rasterToPoints(image))
names(mydata) <- c("x", "y", "type")
pts <- as.matrix(mydata[,1:2])
# cbd
cbd <-  readOGR(dsn = newdir, layer = paste(city1, "_CBD_Project", sep=""))
cbdpoint <- SpatialPoints(cbd)
# distance to the cbd
mydata$cbd_dist <- spDistsN1(pts, cbdpoint, longlat = F)
# library('Hmisc') # cut2
mydata$ring <- as.numeric(cut2(mydata$cbd_dist,g = 100))
mydata$type <- as.factor(mydata$type)
mydata$ring <- as.factor(mydata$ring)
sat_output <- aggregate(mydata$type, by = list(mydata$ring), FUN = get_sat)
sat_output$City <- city1
sat_output$City_Saturation <- get_sat(mydata$type)
names(sat_output) <- c("ring", "ring_saturation","city", "city_saturation")
sat_out <- rbind(sat_out, sat_output)
# draw the figure ---------------------------------------------------------
mydata2 <- join(mydata, sat_output, by = "ring")
r1 <- rasterFromXYZ(mydata2)
# writeRaster(r1, paste(raster_dir, city1, sep = ""), format = "HFA", overwrite=TRUE)
# To preview the color panel:
# display.brewer.pal(n = 12, name = 'RdYlGn')
color_scale <- brewer.pal(n = 10, name = "RdYlGn")
myPalette <- colorRampPalette(rev(color_scale))
plot(r1$ring_saturation, col=myPalette(70), breaks = c(30:100)/100,
legend=FALSE,  main = city1)
plot(cbdpoint, add = TRUE, size = 3) # drawn on top of a map using `add = TRUE`
}
blogdown:::serve_site()
library(blogdown)
build_site()
blogdown:::serve_site()
setwd("D:/OneDrive/R_Blog_Github/Blogdown/hugo-xmag")
library(blogdown)
build_site()
