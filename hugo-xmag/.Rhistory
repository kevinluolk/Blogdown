eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = T)
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
## functions for plot
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xgb_model, shap_approx = TRUE,
X_train = cwv_data$train_mm)
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
# Column Water Vapor correction - Modified script for correcting AOD measurements
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(googledrive)
library(data.table)
library(xgboost)
options(digits = 6)
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
stringsAsFactors = TRUE)
# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata))
# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]
# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
"CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]
# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
names(mydata)[grepl("Structure", names(mydata))],
names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")
# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)
# The model.matrix() function is used in many regression packages for building
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,]
data_test <- data.frame(X2, Y2)[-train_idx,]
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xgb_model, shap_approx = TRUE,
X_train = cwv_data$train_mm)
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = data_train)
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = X_train)
print(shap_result$mean_shap_score)
shap_long <- shap.prep(X_train = cwv_data$train_mm, top_n = dim(cwv_data$train_mm)[2])
shap_long <- shap.prep(X_train = X_train, top_n = dim(X_train)[2])
# make summary plot
plot.shap.summary(data_long = shap_long)
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
eval_metric = "rmse",      # rmse is used for regression
max_depth = 9,
eta = 0.09822,   # Learning rate, default: 0.3
subsample = 0.64,
colsample_bytree = 0.6853,
min_child_weight = 6, # These two are important
max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)
## functions for plot
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm,
top_n = 48){
# descending order
if (is.null(shap)) shap <- shap.score.rank()
shap_score_sub <- shap$shap_score[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
# print(head(shap_long2))
return(shap_long2)
}
# plot for one feature
# aothat.terra aothat.aqua aod.terra aod.aqua
plot.shap.feature <- function(show_feature = "aothat.terra", data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
# plot2 <- ggMarginal(plot1, type = "histogram", bins = 100)
print(plot1)
summary0 <- summary(data_long$value[data_long$variable == show_feature])
return(list(figure = show_feature, summary = summary0))
}
# simplified version for markdown
plot.shap.feature2 <- function(show_feature, data_long = shap_long){
plot1 <- ggplot(data = data_long[variable == show_feature],
aes(x = rfvalue, y = value))+
geom_point(size = 0.1, color = "blue2", alpha = 0.5)+
theme_classic() +
labs(y = "SHAP", x = show_feature) +
geom_hline(yintercept = 0, lwd = 0.2)
return(plot1)
}
# make summary plot (sina plot)
plot.shap.summary <- function(data_long = shap_long){
require('ggplot2')
x_bound <- max(abs(data_long$value))
# jitter <- position_jitter(height = 0.1)
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long, aes(x = variable, y = value, color = stdfvalue))+
coord_flip() +
geom_sina() +
# geom_jitter(alpha = 0.5, position = position_jitter(height = 0.1, width = 0.1)) +
scale_color_gradient(low="green", high="red",
breaks=c(0,1), labels=c("Low","High")) +
theme_classic() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="right") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
scale_x_discrete(limits = rev(levels(data_long$variable))) + # reverse the order
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
print(plot1)
}
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = TRUE,
X_train = X_train)
print(shap_result$mean_shap_score)
shap_long <- shap.prep(X_train = X_train, top_n = dim(X_train)[2])
# make summary plot
plot.shap.summary(data_long = shap_long)
# make summary plot
plot.shap.summary(data_long = shap_long)
install.packages('ggforce')
# make summary plot
plot.shap.summary(data_long = shap_long)
shap.score.rank
shap_long <- shap.prep(X_train = X_train, top_n = 10)
# make summary plot
plot.shap.summary(data_long = shap_long)
f_ranked <- names(shap_result$mean_shap_score)
xgb.plot.shap(data = cwv_data$train_mm, model = xgb_model, features = f_ranked[1:9], n_col = 3)
xgb.plot.shap(data = X_train, model = xg_mod, features = f_ranked[1:9], n_col = 3)
print(shap_result$mean_shap_score)
print(shap_result$mean_shap_score)[1:10]
# serve_site()
build_site()
# serve_site()
build_site()
library(blogdown)
# serve_site()
build_site()
# serve_site()
build_site()
serve_site()
servr::daemon_stop("564274664")
build_site()
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(randomForest)
？rfeControl
?rfeControl
qnorm(0.05)
qnorm(0.025)
qnorm(0.005)
p = 54/120
p
p(1-p)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
sd(apply(matrix(runif(10000),1000),1,mean))
sd(apply(matrix(rnorm(10000),1000),1,mean))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
# returns the CI of binomial:
binom.test(60,100)$conf.int
x = 54
n = 120
p = x/n
p + c(-1,1)*sqrt(p*(1-p)/n)
# returns the CI of binomial:
binom.test(60,100)$conf.int
# returns the CI of binomial:
binom.test(60,100)
# returns the CI of binomial:
binom.test(54,120)
# returns the CI of binomial:
binom.test(54,120)$conf.int
p + c(-1,1)*sqrt(p*(1-p)/n)*1.96
p + c(-1,1)*qnorm(0.975)*sqrt(p*(1-p)/n)
histfun <- function(what,breaks=44,mainn,xlim=c(1,3)){
cexx=1.4;lwdd=4
hist(beta,breaks=44,freq=F,xlim=c(1,3),main=paste(mainn, ", #Obs = ", TT,sep=""),
cex.axis=cexx,cex.main=cexx,cex.lab=cexx-.2,col='lightblue')
den <- density(what)
abline(v=2,col=3,lwd=lwdd)
abline(v=mean(what),col=2,lwd=lwdd)
lines(den,col=2,lwd=lwdd)
}
###  Biased But Consistent
repet = 1000
TT <- 20
beta <- NULL
for (i in 1:repet){
x <- rnorm(TT)
eps <- rnorm(TT,0,1)
y=2+2*x+eps
beta[i] <- (10/TT)+lm(y~x)$coef[2] # Text book example
}
histfun(beta,mainn="Biased")
TT <- 1000
for (i in 1:repet){
x <- rnorm(TT)
eps <- rnorm(TT,0,1)
y=2+2*x+eps
beta[i] <- (10/TT)+lm(y~x)$coef[2]
}
histfun(beta,mainn="Consistent")
# power for 2-sample mean
muA=0
muB=5
kappa=1
sd=10
alpha=0.05
beta=0.20
(nB=(1+1/kappa)*(sd*(qnorm(1-alpha/2)+qnorm(1-beta))/(muA-muB))^2)
ceiling(nB) # 63
z=(muA-muB)/(sd*sqrt((1+1/kappa)/nB))
(Power=pnorm(z-qnorm(1-alpha/2))+pnorm(-z-qnorm(1-alpha/2)))
z0=1.96*sd/sqrt(31.5)
z0
z1=(z0-5)/sd*sqrt(31.5)
pnorm(10-z0, mean=5, sd=10/sqrt(31.5) )
1-pnorm(z1)
pnorm(-1)
pnorm(-2)
pnorm(-1.96)
qnorm(0.025)
qnorm(0.005)
pnorm(-2.576)
qnorm(0.05)
pnorm(-1.645) #
pnorm(-3)
pnorm(-3)*2
# returns the CI of binomial:
binom.test(46, 450)$conf.int
binom.test(46, 450, p = 0.12)
(2.576 / z)^2 * p * (1-p)
# sample size?
p = 0.12
z = 0.01
(2.576 / z)^2 * p * (1-p)
# Mean --------------------------------------------------------------------
qt(0.05, df = 15)
110 + c(-1,1)* 1.753 * 14 / 4
2.22*2.576
(2.22*2.576)^2
33.3*0.9
var(c(1:5 * 10))
c(1:5 * 10)
var(c(1:5 * 10))
?var
var(c(1:5))
250/sqrt(2)
?combn
(10810/6) - 45 * 230/6
((10810/6) - 45 * 230/6)/(12658/6 - 45^2)
2000+3600+2000
2000+3600+2000 - 1500-250-300
2976 / 37.4 - 2578 / 31.2
2698/32.3
2148/430
430/15.7
545/18.8
log(701578/350789)
log(701578/350789)/15
log(701578/350789)/25
6205916000*2 + 23915 * 3
(16000*2 + 23915 * 3)/59/2
59/2
883/807
883/807/22.7*28.4
