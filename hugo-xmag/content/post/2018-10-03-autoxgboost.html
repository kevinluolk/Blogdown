---
title: 'autoxgboost: Automatic XGBoost using Bayesian Optimization'
author: Yang Liu
date: '2018-10-03'
slug: autoxgboost-bayesian-optimization
categories:
  - Machine Learning
  - Parameter Tuning
tags:
  - XGBoost
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#background">Background</a></li>
<li><a href="#using-autoxgboost">Using <code>autoxgboost</code></a></li>
<li><a href="#new-result">New Result</a><ul>
<li><a href="#compared-to-old-result">Compared to old Result</a></li>
<li><a href="#tuning-over-different-boosters">Tuning over Different Boosters</a></li>
<li><a href="#return-the-recommendedchosen-parameters">Return the recommended/chosen parameters</a></li>
</ul></li>
</ul>
</div>

<p>(updated on Oct 14)</p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>It has been a while since my last update. I have been working on lots of interesting projects since I joined Mount Sinai in August. We have a great team here and obviously I can learn a lot from everyone around me. Most of my job so far focuses on applying machine learning techniques, mainly extreme gradient boosting and the visualization of results. Parameter tuning could be challenging in XGBoost. I recently tried <code>autoxgboost</code>, which is so easy to use and runs much faster than the naive grid or random search illustrated in my <a href="https://liuyanguu.github.io/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/">earlier post on XGBoost</a>. The results are also as good as the best effort we could obtain from the time-consuming random search.</p>
<p>I use the same dataset to exemplify <code>autoxgboost</code><br />
To install the package, run <em><code>devtools::install_github("ja-thomas/autoxgboost")</code></em></p>
</div>
<div id="using-autoxgboost" class="section level1">
<h1>Using <code>autoxgboost</code></h1>
<ul>
<li>A paper on <em><a href="https://arxiv.org/pdf/1807.02811.pdf">Bayesian Optimization</a></em><br />
</li>
<li>A presentation: <em><a href="http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf">Introduction to Bayesian Optimization</a></em><br />
</li>
<li>By default, the optimizer runs for for 160 iterations or 1 hour, results using 80 iterations are good enough<br />
</li>
<li>By default, <code>par.set</code>: parameter set to tune over, is <code>autoxgbparset</code>:</li>
</ul>
<pre class="r"><code>autoxgbparset</code></pre>
<pre><code>##                      Type len Def      Constr Req Tunable Trafo
## eta               numeric   -   - 0.01 to 0.2   -    TRUE     -
## gamma             numeric   -   -     -7 to 6   -    TRUE     Y
## max_depth         integer   -   -     3 to 20   -    TRUE     -
## colsample_bytree  numeric   -   -    0.5 to 1   -    TRUE     -
## colsample_bylevel numeric   -   -    0.5 to 1   -    TRUE     -
## lambda            numeric   -   -   -10 to 10   -    TRUE     Y
## alpha             numeric   -   -   -10 to 10   -    TRUE     Y
## subsample         numeric   -   -    0.5 to 1   -    TRUE     -</code></pre>
<ul>
<li>This dataset is a regression problem, for classification, use <code>makeClassifTask</code> instead of <code>makeRegrTask</code> in the <code>makeRegrTask</code> function. There are more options for different tasks<br />
</li>
<li>Use all as default, input a <em>data.frame</em>, and that’s it…</li>
</ul>
<pre class="r"><code>library(autoxgboost)
reg_task &lt;- makeRegrTask(data = data_train, target = &quot;Share_Temporary&quot;)
set.seed(1234)
system.time(reg_auto &lt;- autoxgboost(reg_task))
# saveRDS(reg_auto, file = &quot;D:/SDIautoxgboost_80.rds&quot;)</code></pre>
</div>
<div id="new-result" class="section level1">
<h1>New Result</h1>
<pre><code>## Autoxgboost tuning result
## 
## Recommended parameters:
##               eta: 0.118
##             gamma: 0.035
##         max_depth: 7
##  colsample_bytree: 0.860
## colsample_bylevel: 0.671
##            lambda: 7.731
##             alpha: 0.236
##         subsample: 0.642
##           nrounds: 57
## 
## 
## Preprocessing pipeline:
## dropconst(rel.tol = 1e-08, abs.tol = 1e-08, ignore.na = FALSE)
## 
## With tuning result: mse = 0.044</code></pre>
<ul>
<li>Testing mse: 0.047 (rmse is: 0.2168) is quite close to the 0.043 from previous post. But it is much faster using only 57 rounds. Notice that the cross-valiation tuning mse is almost the same: 0.044.</li>
</ul>
<pre><code>## [1] 0.0469821</code></pre>
<div id="compared-to-old-result" class="section level2">
<h2>Compared to old Result</h2>
<ul>
<li>Parameters</li>
</ul>
<pre><code>##   nrounds max_depth   eta gamma colsample_bytree min_child_weight
## 1     228         8 0.034     0           0.7208                7
##   subsample
## 1    0.7017</code></pre>
<ul>
<li>rmse</li>
</ul>
<pre class="r"><code># 0.0433</code></pre>
</div>
<div id="tuning-over-different-boosters" class="section level2">
<h2>Tuning over Different Boosters</h2>
<ul>
<li><code>autoxgboost</code> also allows us to tune over the three types of boosters: <em>gbtree, gblinear and dart</em></li>
<li>The paraset <code>autoxgbparset.mixed</code> was predifined by author, but it seems I still need to load it<br />
</li>
<li>Here is the <a href="https://github.com/ja-thomas/autoxgboost/issues/60#issuecomment-428991564">question I consulted on github</a></li>
</ul>
<pre class="r"><code>reg_task &lt;- makeRegrTask(data = data_train, target = &quot;Share_Temporary&quot;)

autoxgbparset.mixed = makeParamSet(
  makeDiscreteParam(&quot;booster&quot;, values = c(&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;)),
  makeDiscreteParam(&quot;sample_type&quot;, values = c(&quot;uniform&quot;, &quot;weighted&quot;), requires = quote(booster == &quot;dart&quot;)),
  makeDiscreteParam(&quot;normalize_type&quot;, values = c(&quot;tree&quot;, &quot;forest&quot;), requires = quote(booster == &quot;dart&quot;)),
  makeNumericParam(&quot;rate_drop&quot;, lower = 0, upper = 1, requires = quote(booster == &quot;dart&quot;)),
  makeNumericParam(&quot;skip_drop&quot;, lower = 0, upper = 1, requires = quote(booster == &quot;dart&quot;)),
  makeLogicalParam(&quot;one_drop&quot;, requires = quote(booster == &quot;dart&quot;)),
  makeDiscreteParam(&quot;grow_policy&quot;, values = c(&quot;depthwise&quot;, &quot;lossguide&quot;)),
  makeIntegerParam(&quot;max_leaves&quot;, lower = 0, upper = 8, trafo = function(x) 2^x, requires = quote(grow_policy == &quot;lossguide&quot;)),
  makeIntegerParam(&quot;max_bin&quot;, lower = 2L, upper = 9, trafo = function(x) 2^x),
  makeNumericParam(&quot;eta&quot;, lower = 0.01, upper = 0.2),
  makeNumericParam(&quot;gamma&quot;, lower = -7, upper = 6, trafo = function(x) 2^x),
  makeIntegerParam(&quot;max_depth&quot;, lower = 3, upper = 20),
  makeNumericParam(&quot;colsample_bytree&quot;, lower = 0.5, upper = 1),
  makeNumericParam(&quot;colsample_bylevel&quot;, lower = 0.5, upper = 1),
  makeNumericParam(&quot;lambda&quot;, lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam(&quot;alpha&quot;, lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam(&quot;subsample&quot;, lower = 0.5, upper = 1)
)
system.time(reg_auto_dart &lt;- autoxgboost(reg_task, par.set = autoxgbparset.mixed))</code></pre>
<ul>
<li>Interestingly enough, a model with dart booster has been chosen, but the results are pretty much the same. Tuning mse = 0.044, and testing mse = 0.048. It means a result around 0.044 is about the best we can achieve through <code>xgboost</code>.</li>
</ul>
<pre><code>## Autoxgboost tuning result
## 
## Recommended parameters:
##           booster: dart
##       sample_type: weighted
##    normalize_type: forest
##         rate_drop: 0.635
##         skip_drop: 0.765
##          one_drop: TRUE
##       grow_policy: lossguide
##        max_leaves: 1
##           max_bin: 16
##               eta: 0.051
##             gamma: 0.086
##         max_depth: 10
##  colsample_bytree: 0.838
## colsample_bylevel: 0.544
##            lambda: 0.009
##             alpha: 0.003
##         subsample: 0.726
##           nrounds: 54
## 
## 
## Preprocessing pipeline:
## dropconst(rel.tol = 1e-08, abs.tol = 1e-08, ignore.na = FALSE)
## 
## With tuning result: mse = 0.044</code></pre>
<pre><code>## testing mse:</code></pre>
<pre><code>## [1] 0.0487725</code></pre>
</div>
<div id="return-the-recommendedchosen-parameters" class="section level2">
<h2>Return the recommended/chosen parameters</h2>
<ul>
<li>By the way, since <code>autoxgboost</code> was built on <code>mlr</code> package, it might appear difficult to further engineer the output, for example, extract parameters for further use. To extract the the tuned parameters using <code>getHyperPars</code> from the <code>mlr</code> package:</li>
</ul>
<pre class="r"><code>Param_chosen &lt;- mlr::getHyperPars(reg_auto_dart$final.learner)
print(unlist(Param_chosen))</code></pre>
<pre><code>##               nrounds               verbose             objective 
##                  &quot;54&quot;                   &quot;0&quot;          &quot;reg:linear&quot; 
##               booster           sample_type        normalize_type 
##                &quot;dart&quot;            &quot;weighted&quot;              &quot;forest&quot; 
##             rate_drop             skip_drop              one_drop 
##   &quot;0.634519323528358&quot;   &quot;0.765370830696091&quot;                &quot;TRUE&quot; 
##           grow_policy            max_leaves               max_bin 
##           &quot;lossguide&quot;                   &quot;1&quot;                  &quot;16&quot; 
##                   eta                 gamma             max_depth 
##  &quot;0.0507358974376265&quot;  &quot;0.0863826961866801&quot;                  &quot;10&quot; 
##      colsample_bytree     colsample_bylevel                lambda 
##   &quot;0.837704133330852&quot;   &quot;0.543820898304747&quot;  &quot;0.0090610428548379&quot; 
##                 alpha             subsample 
## &quot;0.00319982004364013&quot;   &quot;0.726191523257774&quot;</code></pre>
</div>
</div>
