---
title: 'SHAP Visualization for XGBoost'
author: Yang Liu
date: '2018-10-05'
slug: shap-visualization-for-xgboost
categories:
  - Machine Learning
  - Data Visualization
tags:
  - XGBoost
  - SHAP
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```
# Background
I will illustrate here the application of SHAP (SHapley Additive exPlnation) values to visualize the efforts of features on the outcome variable in a XGBoost model.
The function was developed by _Scott Lundberg_ in Python [Github Link](https://github.com/slundberg/shap) and then combined into _xgboost_ with one visualization function _xgb.plot.shap_. But we can make better summary figures as those functions in its Python package in more flexible ways by extracting the SHAP values and plot by ourselves. 

```{r libs, include = FALSE}
# Column Water Vapor correction - Modified script for correcting AOD measurements 
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(googledrive)
library(data.table)
library(xgboost)
options(digits = 6)
```

```{r data, include = FALSE}
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                stringsAsFactors = TRUE)

# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata)) 

# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]

# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
                 "CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]

# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
                       names(mydata)[grepl("Structure", names(mydata))],
                       names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")

# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)

# The model.matrix() function is used in many regression packages for building 
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,] 
data_test <- data.frame(X2, Y2)[-train_idx,] 
```

# Original Result  
* Parameters  
```{r}
tunegrid <- structure(list(nrounds = 228, max_depth = 8, eta = 0.034, gamma = 0, 
                           colsample_bytree = 0.7208, min_child_weight = 7, subsample = 0.7017), 
                           Names = c("nrounds", 
                                 "max_depth", "eta", "gamma", "colsample_bytree", "min_child_weight", 
                                 "subsample"), row.names = 1L, class = "data.frame")

tunegrid
```
* rmse   
```{r, echo = T}
# 0.0433
```

# Using `autoxgboost` 
* A paper on _[Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf)_  
* A presentation: _[Introduction to Bayesian Optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf)_  
* By default, the optimizer runs for for 160 iterations or 1 hour, results using 80 iterations are good enough    
* By default, `par.set`: parameter set to tune over, is `autoxgbparset`:  
```{r echo = T}
autoxgbparset
```
* This dataset is a regression problem, for classification, just use `reg_task <- makeClassifTask`. There are more options for different tasks  
* Use all as default, input a _data.frame_, and that's it...   
```{r, echo = T, eval = F}
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
# If need to change iterations or control, all are very easy:
# MBOctrl <- makeMBOControl()
# ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
# system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
# saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
```
 

# New Result 
```{r}
library(googledrive)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
temp <- tempfile(fileext = ".rds")
drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
reg_auto <- readRDS(temp)
print(reg_auto)
```
* Testing rmse: 0.1097 compared to 0.043, it is actually much worse than the result from random search (in my dataset from work, `autoxgboost` did no worse). I suspect this is because the number of observations is not abundant compared to the number of features (316 x 88).
```{r}
xgb_pred <- predict(reg_auto, data_test) 
sqrt(mean((xgb_pred$data$response - Y_test))) 

```
