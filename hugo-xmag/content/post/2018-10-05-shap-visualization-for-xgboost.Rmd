---
title: 'SHAP Visualization for XGBoost'
author: Yang Liu
date: '2018-10-14'
slug: shap-visualization-for-xgboost
categories:
  - Machine Learning
  - Data Visualization
tags:
  - XGBoost
  - SHAP
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```
(Under further revise)

# Background
I will illustrate the application of SHAP (SHapley Additive exPlnation) values to visualize the efforts of features on the outcome variable in a XGBoost model.  
The function was developed by [_Scott Lundberg_ in Python and explained here](https://github.com/slundberg/shap). SHAP was combined into _xgboost_ with one visualization function _xgb.plot.shap_. But we can make better summary figures as in the Python package in more flexible ways by extracting the SHAP values. 

```{r libs, include = FALSE}
# Column Water Vapor correction - Modified script for correcting AOD measurements 
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(googledrive)
library(data.table)
library(xgboost)
library(ggforce)
library(ggplot2)
options(digits = 4)

# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                stringsAsFactors = TRUE)

# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata)) 

# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]

# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
                 "CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]

# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
                       names(mydata)[grepl("Structure", names(mydata))],
                       names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")

# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)

# The model.matrix() function is used in many regression packages for building 
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,] 
data_test <- data.frame(X2, Y2)[-train_idx,] 
```

# Summary plot  
* Using `geom_sina` from `ggforce` to make the sina plot  
* We can see clearly for the most influential variable on the top: Monthly water cost. A Higher cost is associated with the declined share of temporary housing. But a very low cost has a strong impact on the increased share of temporary housing  
* The effects of binary variables are highly distinctive. The second variable shows that Resettled housing is highly unlikely to be temporary, so does being close to wells as water sources.  
   
__Load the xgboost model__  
```{r, echo = T, fig.width = 6, fig.height = 6}
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
                eval_metric = "rmse",      # rmse is used for regression
                max_depth = 9,
                eta = 0.09822,   # Learning rate, default: 0.3
                subsample = 0.64,
                colsample_bytree = 0.6853, 
                min_child_weight = 6, # These two are important
                max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
xg_mod <- xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)

```
  
### Function to get SHAP value matrix  
__Also need it for the stacked plot for observation__  
```{r, echo = T}
## not necessary to use these functions... I just wrapped them up...
## return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE, 
                            X_train){
  require(xgboost)
  # `shap_contrib` returns the matrix of SHAP values: dim is (Number of obs)x(Number of Features)
  shap_contrib <- predict(xgb_model, X_train,
                          predcontrib = TRUE, approxcontrib = shap_approx)
  shap_contrib <- as.data.table(shap_contrib)
  shap_contrib[,BIAS:=NULL]
  # rank SHAP score by decreasing order:
  mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
  return(list(shap_score = shap_contrib,
              mean_shap_score = (mean_shap_score)))
}

```
  
### Functions for summary plot  
__To get it work, please install 'ggforce' from github, as there was a bug in the new `geom_sina`__  
Now in `geom_sina` the default method is "density", to do it my originally way, need to change into `method = "counts", maxwidth = 0.7`  
  
```{r, eval = F}

devtools::install_github("thomasp85/ggforce")

```
  
```{r}
packageVersion("ggforce")

## a function to standardize feature values into same range
std1 <- function(x){
  return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}

## prep shap data into long format, option to plot only `top_n` features  
shap.prep <- function(shap  = shap_result, X_train, top_n){
  # descending order
  if (missing(top_n)) top_n <- dim(X_train)[2] # by default, use all features
  if (!top_n%in%c(1:dim(X_train)[2])) stop('supply correct top_n')
  require(data.table)
  shap_score_sub <- as.data.table(shap$shap_score)
  shap_score_sub <- shap_score_sub[, names(shap$mean_shap_score)[1:top_n], with = F]
  shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
  
  # feature values: the values in the original dataset
  fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
  # standardize feature values
  fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
  fv_sub_long[, stdfvalue := std1(value), by = "variable"]
  # SHAP value: value
  # raw feature value: rfvalue; 
  # standarized: stdfvalue
  names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
  shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
  shap_long2[, mean_value := mean(abs(value)), by = variable]
  setkey(shap_long2, variable)
  return(shap_long2) 
}

## the function to plot the summary plot:
plot.shap.summary <- function(data_long){
  x_bound <- max(abs(data_long$value))
  require('ggplot2')
  require('ggforce') # for `geom_sina`
  plot1 <- ggplot(data = data_long)+
    coord_flip() + 
    # sina plot: 
    geom_sina(aes(x = variable, y = value, color = stdfvalue),
              method = "counts", maxwidth = 0.7) +
    # print the mean absolute value on the left: 
    geom_text(data = unique(data_long[, c("variable", "mean_value"), with = F]),
              aes(x = variable, y=-Inf, label = sprintf("%.3f", mean_value)),
              size = 3, alpha = 0.7,
              hjust = -0.2, 
              fontface = "bold") + # bold
    # # if want to add a "SHAP" bar notation
    # annotate("text", x = -Inf, y = -Inf, vjust = -0.2, hjust = 0, size = 3,
    #          label = expression(group("|", bar(SHAP), "|"))) + 
    scale_color_gradient(low="#FFCC33", high="#6600CC", 
                         breaks=c(0,1), labels=c("Low","High")) +
    scale_y_continuous(limits = c(-x_bound, x_bound)) +
    # reverse the order of features
    scale_x_discrete(limits = rev(levels(data_long$variable))) + 
    geom_hline(yintercept = 0) + # the vertical line
    labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value") +
    theme_bw() + 
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
          legend.position="bottom")
  return(plot1)
}

## Prepare data:
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = F, 
                               X_train = X_train)
shap_long <- shap.prep(X_train = X_train, top_n = 10)
## The data prepared: variable, SHAP value, raw feature value, standarded feature value, mean value by feature
print(shap_long)
## make summary plot
plot.shap.summary(data_long = shap_long)

```
  
# SHAP plot for each feature  
* Use the built-in `xgb.plot.shap` function
* Here we choose to show top 9 features ranked by impact on the dependent variable  
```{r, echo = T}
f_ranked <- names(shap_result$mean_shap_score)
xgb.plot.shap(data = X_train, model = xg_mod, features = f_ranked[1:6], n_col = 3)
```
  
# SHAP stack plot for observation  
* Use `geom_col` to "show features each contributing to push the model output from the base value (the average model output) to the model output."  
* Have tried `geom_area` but dones't work very well due to gaps in the plot caused by fluctuation of positive and negative values  
* Use the order of clustering to group observations under similar influences closer  
       
### Function for stack plot   
```{r, fig.width = 8, fig.height = 6}
# SHAP stack plot ---------------------------------------------------------
# shap_contrib is the SHAP value matrix returned from predict, top_n features, option to randomly plot p*100 percent of the data in case the dataset is large 
shap.stack.data <- function(shap_contrib, top_n, data_percent = 1){
  require(data.table)
  ranked_col <- names(colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)])
  shap_contrib2 <- setDT(shap_contrib)[,..ranked_col]
  
  # sample `data_percent` of the data
  set.seed(1234)
  if (data_percent>1) stop("data_percent <= 1")
  shap_contrib2 <- shap_contrib2[sample(.N, .N * data_percent)]
  
  # select columns    
  if (top_n < length(ranked_col)){
    top_ranked_col <- ranked_col[1:top_n]
    bottom_col <- ranked_col[-(1:top_n)]
    shap_contrib2[, rest_variables:= rowSums(.SD), .SDcol = bottom_col]
    # dataset with desired variables for plot
    shapobs <- shap_contrib2[, c(top_ranked_col, "rest_variables"), with = F]
  } else if (top_n == length(ranked_col)) {
    shapobs <- shap_contrib2
  } else {stop(paste("top_n should not above", length(ranked_col)))}
  
  
  # sort by cluster
  clusters <- hclust(dist(scale(shap_contrib2)), method = "ward.D")
  # re-arrange the rows accroding to dendrogram
  shapobs <- shapobs[, clusterid:= clusters$order][rank(clusterid),]
  shapobs[,clusterid:=NULL]
  shapobs[,id:=.I]
  return(shapobs)
}

## make the stack plot
shap.stack.plot <- function(shapobs, id = 'id',
                            zoom_in_location = NULL, # where to zoom in, default at place of 60% of the data
                            y_parent_limit = NULL,
                            y_limit = NULL,          # c(a,b) to limit the y-axis in zoom-in
                            zoom_in = TRUE){         # option to close zoom-in
  require(data.table)
  require(ggplot2)
  require(ggforce)  
  # optional: location to zoom in certain part of the plot
  shapobs_long <- melt.data.table(shapobs, id.vars = id)
  require(RColorBrewer)
  # display.brewer.pal("Paired")
  p <- ggplot(shapobs_long, aes_string(x = id, y = "value" , fill = "variable")) + 
    geom_col(width =1, alpha = 0.9) +
    # geom_area() +
    labs(fill = 'Feature', x = 'Observation', 
         y = 'SHAP values by feature:\n (Contribution to the base value)') + 
    geom_hline(yintercept = 0, col = "gray40") + 
    theme_bw() + 
    scale_y_continuous(limits = y_parent_limit)
  # theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
  
  # how to apply color
  if (dim(shapobs)[2]-1<=12){p <- p +    
    # [notes] yes this discrete palette has a maximum of 12
    scale_fill_manual(values = brewer.pal(dim(shapobs)[2]-1, 'Paired'))
  } else {
    p <- p +  scale_fill_viridis_d(option = "D")  # viridis color scale
  }
  
  # how to zoom in
  if(zoom_in){
    x_mid <- if (is.null(zoom_in_location)) shapobs[,.N]*0.6 else zoom_in_location
    x_interval <- median(c(50, 150, floor(shapobs[,.N]*0.1)))
    cat("Data has N:", shapobs[,.N],"| zoom in length is", x_interval, "at location",x_mid, "\n")
    p <- p + 
      facet_zoom(xlim = c(x_mid, x_mid + x_interval),
                 ylim = y_limit, horizontal = F
      ) +
      theme(zoom.y = element_blank(), validate = FALSE) # zoom in using this line
  } else {
    cat("Data has N:", shapobs[,.N],"| no zoom in\n")
  }
  return(p)
}


# use the SHAP value matrix created above and stored in object `shap_result`  
## Prepare data:
shap_result <- shap.score.rank(xgb_model = xg_mod, shap_approx = F, 
                               X_train = X_train)
shapobs <- shap.stack.data(shap_contrib = shap_result$shap_score, top_n = 7, data_percent = 1)
shap.stack.plot(shapobs = shapobs,
                zoom_in_location  = 50,
                # y_parent_limit = c(-1,1),
                y_limit = c(-0.3, 0.5),
                zoom_in = F)
```

  
```{r,eval = F}
library("reticulate")
py_install('jupyter')

write.csv(cbind(X_train, Y_train), "D:/liuyanguu/Blogdown/shapdata.csv")

```

```{python, eval = F}
import pandas
import xgboost
import shap

data = pandas.read_csv("D:/liuyanguu/Blogdown/shapdata.csv")
data_Y = data[data.columns[-1]]
data_X = data[data.columns[2:data.shape[1]-1]]
data_X.head(1)

# train XGBoost model
model = xgboost.train(params = {"learning_rate": 0.09822,
                      "max_depth": 9,
                       "subsample": 0.64,
                       "colsample_bytree": 0.6853,
                       "min_child_weight": 6,
                       "max_delta_step" :8
                      }, 
                      dtrain = xgboost.DMatrix(data_X, label=data_Y), 
                      num_boost_round = 3660)


# explain the model's predictions using SHAP values
# (same syntax works for LightGBM, CatBoost, and scikit-learn models)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(data_X)

# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value, shap_values, data_X)

import pandas as pd
import numpy as np
import scipy as sp
from scipy.spatial.distance import pdist
    
D = sp.spatial.distance.pdist(shap_values, metric="sqeuclidean")
cluster_matrix = sp.cluster.hierarchy.complete(D)
cluster_matrix[0:3,]
cluster_matrix
metric="sqeuclidean" 
X = shap_values
sets = [[i] for i in range(X.shape[0])]
for i in range(cluster_matrix.shape[0]):
        s1 = sets[int(cluster_matrix[i,0])]
        s2 = sets[int(cluster_matrix[i,1])]
        
        # compute distances between thse end points of the lists
        d_s1_s2 = pdist(np.vstack([X[s1[-1],:], X[s2[0],:]]), metric)[0]
        d_s2_s1 = pdist(np.vstack([X[s1[0],:], X[s2[-1],:]]), metric)[0]
        d_s1r_s2 = pdist(np.vstack([X[s1[0],:], X[s2[0],:]]), metric)[0]
        d_s1_s2r = pdist(np.vstack([X[s1[-1],:], X[s2[-1],:]]), metric)[0]

        # concatenete the lists in the way the minimizes the difference between
        # the samples at the junction
        best = min(d_s1_s2, d_s2_s1, d_s1r_s2, d_s1_s2r)
        if best == d_s1_s2:
            sets.append(s1 + s2)
        elif best == d_s2_s1:
            sets.append(s2 + s1)
        elif best == d_s1r_s2:
            sets.append(list(reversed(s1)) + s2)
        else:
            sets.append(s1 + list(reversed(s2)))
            
            sets[-1]
```

