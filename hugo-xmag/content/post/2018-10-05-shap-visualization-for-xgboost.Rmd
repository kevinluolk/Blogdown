---
title: 'SHAP Visualization in R (first post)'
author: Yang Liu
date: '2018-10-14'
slug: shap-visualization-for-xgboost
categories:
  - Machine Learning
  - Data Visualization
tags:
  - XGBoost
  - SHAP
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

19/07/21:
An updated version of these functions has been posted in a later post. I used a package to wrap them up. Please refer to: [SHAP visualization for XGBoost in R](https://liuyanguu.github.io/post/2019/07/18/visualization-of-shap-for-xgboost/)

# Background
I will illustrate the application of SHAP (SHapley Additive exPlnation) values to visualize the efforts of features on the outcome variable in a XGBoost model.  
The function was developed by [_Scott Lundberg_ in Python and explained here](https://github.com/slundberg/shap). SHAP was combined into _xgboost_ with one visualization function _xgb.plot.shap_. But we can make better summary figures as in the Python package in more flexible ways by extracting the SHAP values. 

# Example 1

This is the simple example I used in the package **SHAPforxgboost**

```{r}
# Example use iris
library(SHAPforxgboost)
X1 = as.matrix(iris[,-5])
mod1 = xgboost::xgboost(
  data = X1, label = iris$Species, gamma = 0, eta = 1, lambda = 0,nrounds = 1, verbose = F)

# shap.values() has the SHAP data matrix and ranked features by mean|SHAP|
shap_values <- shap.values(mod1, X1)
# ranked features:
shap_values$mean_shap_score

# shap.prep() returns the long-format SHAP data
shap_long <- shap.prep(shap_values, X1)
# or
shap_long <- shap.prep(shap.values(mod1, X1), X1)
```

## SHAP summary plot

```{r}
# ****
plot.shap.summary(shap_long)

# Alternatives:
# option 1: from the xgboost model
plot.shap.summary.wrap1(mod1, X1, top_n = 3)

# option 2: supply a self-made SHAP values dataset (e.g. sometimes as output from cross-validation)
plot.shap.summary.wrap2(shap_score = shap_values$shap_score, X1, top_n = 3)
```

## SHAP dependence plot

```{r}
# **SHAP dependence plot**
plot.shap.dependence.color(shap_long, x="Petal.Length",
                           y_shap = "Petal.Length", color_feature = "Petal.Width")
```

## SHAP force plot

```{r}
# **SHAP force plot**
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, n_groups = 2)
plot.shap.force_plot(force_plot_data)
# plot.shap.force_plot_bygroup(force_plot_data)
```

# Example 2 

This example is based on the slum data I used in the earilier post.

```{r libs, include = FALSE, echo = FALSE}
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                stringsAsFactors = TRUE)

# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata)) 

# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]

# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
                 "CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]

# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
                       names(mydata)[grepl("Structure", names(mydata))],
                       names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")

# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)

# The model.matrix() function is used in many regression packages for building 
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)
data_train <- data.frame(X2, Y2)[train_idx,] 
data_test <- data.frame(X2, Y2)[-train_idx,] 
```

## Summary plot  
* Using `geom_sina` from `ggforce` to make the sina plot  
* We can see clearly for the most influential variable on the top: Monthly water cost. A Higher cost is associated with the declined share of temporary housing. But a very low cost has a strong impact on the increased share of temporary housing  
* The effects of binary variables are highly distinctive. The second variable shows that Resettled housing is highly unlikely to be temporary, so does being close to wells as water sources.  
   
__Load the xgboost model__  
```{r}
best_rmse_index <- 56
best_rmse <- 0.2102
best_seednumber <- 3660
best_param <- list(objective = "reg:linear",  # For regression
                eval_metric = "rmse",      # rmse is used for regression
                max_depth = 9,
                eta = 0.09822,   # Learning rate, default: 0.3
                subsample = 0.64,
                colsample_bytree = 0.6853, 
                min_child_weight = 6, # These two are important
                max_delta_step = 8)
# The best index (min_rmse_index) is the best "nround" in the model
nround <- best_rmse_index
set.seed(best_seednumber)
mod1 <- xgboost::xgboost(data = X_train, label = Y_train, params = best_param, nround = nround, verbose = F)

```

```{r}
plot.shap.summary.wrap1(mod1, X_train, top_n = 10)
```
  
## Dependence plot for each feature  

* Here we choose to show top 6 features ranked by mean|SHAP|    
```{r, fig.width=10, fig.height=12}
shap_values <- shap.values(xgb_model = mod1, X_train = X_train)
data_long = shap.prep(shap_values, X_train, 6)
features_ranked <- names(shap_values$mean_shap_score)[1:6]

fig_list <- lapply(features_ranked, plot.shap.dependence, data_long = data_long)
gridExtra::grid.arrange(grobs = fig_list, ncol = 2)
```


* If Use the built-in `xgb.plot.shap` function  
```{r, echo = T}
xgboost::xgb.plot.shap(data = X_train, model = mod1, features = features_ranked, n_col = 2)
```
  
  
## Force plot
Since there are so many features in this dataset, we pick only top 6 and merge the rest.   

* Use `geom_col` to show features each contributing to push the model output from the base value (the average model output) to the model output.  
* Have tried geom_area but donesâ€™t work very well due to gaps in the plot caused by fluctuation of positive and negative values.  
* apply the order of clustering to group observations under similar influences closer.  


```{r}
force_plot_data <- shap.stack.data(shap_contrib = shap_values$shap_score, top_n  = 6, n_groups = 4)
plot.shap.force_plot(force_plot_data)

```

## Stack plot by clustering groups
```{r}
plot.shap.force_plot_bygroup(force_plot_data)

```

