---
title: autoxgboost: Bayesian Optimization 
author: Yang Liu
date: '2018-10-03'
slug: autoxgboost
categories:
  - Machine Learning
  - Tuning
tags:
  - XGBoost
output:
  blogdown::html_page:
    toc: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.width = 10, fig.height = 8)
knitr::opts_knit$set(root.dir = "../")
```

It has been a while since I have been working on lots of interesting projects since I joined in Mount Sinai in August. We have a great team here and obviously I can learn a lot from everyone around me. Most of my job so far focuses on extreme gradient boosting and related visualizations. Suggested by Allan, I recently tested `autoxgboost`, which is so easy to use and runs much faster than the naive grid or random search illustrated in my [earlier post on `xgboost`](https://liuyanguu.github.io/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/). The results are also as good as the best effort we could obtain from the time-consuming random search.

I use the same dataset to exemplify 'autoxgboost`.   
* To install the package, run `devtools::install_github("ja-thomas/autoxgboost")`
```{r libs, include = FALSE}
# Column Water Vapor correction - Modified script for correcting AOD measurements 
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
options(digits = 6)
```

```{r data, include = FALSE}
# https://drive.google.com/open?id=1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR
id <- "1-sxXA2oS_SbDxRuh017b4s9PHprKtQFR" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                stringsAsFactors = TRUE)

# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)

# The model.matrix() function is used in many regression packages for building 
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)[,-1]
X2 <- X2[,-1]
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset) / and autoxgboost
data_train <- data.frame(X2, Y2)[train_idx,]; dim(data_train)
data_test <- data.frame(X2, Y2)[-train_idx,];dim(data_test)

```

## Original Result  
* Parameters  
```{r}
tunegrid <- structure(list(nrounds = 228, max_depth = 8, eta = 0.034, gamma = 0, 
                           colsample_bytree = 0.7208, min_child_weight = 7, subsample = 0.7017), 
                           Names = c("nrounds", 
                                 "max_depth", "eta", "gamma", "colsample_bytree", "min_child_weight", 
                                 "subsample"), row.names = 1L, class = "data.frame")

tunegrid
```
* rmse   
```{r, echo = T}
# 0.002841
```

## Using `autoxgboost` 
* A paper on _[Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf)_  
* A presentation: _[Introduction to Bayesian Optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf)_  
* By default, the optimizer runs for for 160 iterations or 1 hour, results using 80 iterations are good enough    
* By default, `par.set`: parameter set to tune over, is `autoxgbparset`:  
```{r echo = T}
autoxgbparset
```
* This dataset is a simple regression problem, for classification, just use `reg_task <- makeClassifTask`  
* There are more options for different tasks  
* Use all as default, input a _data.frame_, and that's it...   
```{r, echo = T, eval = F}
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))

# If need to change iterations or control, all are very easy:
MBOctrl <- makeMBOControl()
ctrl <- setMBOControlTermination(control = MBOctrl, iters = 160L)
system.time(reg_auto <- autoxgboost(reg_task, control = ctrl))
saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")
```
 

### Results 
```{r}
library(googledrive)
# https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
temp <- tempfile(fileext = ".rds")
drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
reg_auto_80 <- readRDS(temp)
print(reg_auto_80)
```
* Testing rmse: 0.0484
```{r}
xgb_pred <- predict(reg_auto_80, data_test) 
sqrt(mean((xgb_pred$data$response - data_test$Share_Temporary)^2)) 

```

### Results for __aqua__  
```{r data2, include = FALSE}
# import prepared dataset
modeldt <- read_fst("/home/liuy29/Jdrive/PM/Just_Lab/projects/ECHO_PM/data/intermediate/MAIACAAOT_MOD1a_CWV_v2018-08-02.fst", as.data.table = TRUE)
# restrict to non-missing
modeldt <- modeldt[!is.na(col_water_cm)]
modeldt <- modeldt[!is.na(AOT_Uncertainty)]
modeldt[, diffcwv := Column_WV - col_water_cm]
set.seed(785959908)
meanobsday <- modeldt[, .N, by = "dayint"][, mean(N)]
daytest <- sample(modeldt[, unique(dayint)], trunc(nrow(modeldt)*0.15)/meanobsday) 
rm(meanobsday)
length(daytest) # 396 days (AQUA) - 454 (TERRA)
traindt <- modeldt[!dayint %in% daytest]
testdt <- modeldt[dayint %in% daytest]
```

```{r}
reg_auto_aqua <- readRDS("~/Intermediate/1002_reg_auto_aqua.rds")
print(reg_auto_aqua)
```
* Testing rmse: 0.1452 (0.1464 for 80 iters) compared to 0.1449
```{r}
xgb_pred <- predict(reg_auto_aqua, as.data.frame(testdt[, ..features])) 
sqrt(mean((testdt$diffcwv - xgb_pred$data$response)^2)) 

```
