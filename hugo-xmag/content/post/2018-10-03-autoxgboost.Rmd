---
title: 'autoxgboost: Automatic XGBoost using Bayesian Optimization'
author: Yang Liu
date: '2018-10-03'
slug: autoxgboost-bayesian-optimization
categories:
  - Machine Learning
  - Parameter Tuning
tags:
  - XGBoost
output:
  blogdown::html_page:
    toc: true
---

(updated on Oct 14)  

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```
# Background

It has been a while since my last update. I have been working on lots of interesting projects since I joined Mount Sinai in August. We have a great team here and obviously I can learn a lot from everyone around me. Most of my job so far focuses on extreme gradient boosting and visualization of results. Suggested by Allan, I recently tested `autoxgboost`, which is so easy to use and runs much faster than the naive grid or random search illustrated in my [earlier post on XGBoost](https://liuyanguu.github.io/post/2018/07/09/extreme-gradient-boosting-xgboost-better-than-random-forest-or-gradient-boosting/). The results are also as good as the best effort we could obtain from the time-consuming random search (when the dataset is large). This short illustration actually produces a counter-example.

<<<<<<< HEAD
I use the same dataset to exemplify `autoxgboost`  
To install the package, run _`devtools::install_github("ja-thomas/autoxgboost")`_
=======
Bayesian optimization (BayesOpt) has become very popular for tuning hyperparameters in machine learning algorithms, especially deep neural network. 

I use the same dataset as in earlier post to exemplify `autoxgboost`  
To install the package, run `devtools::install_github("ja-thomas/autoxgboost")`
>>>>>>> 6d94a88e7140980c26308c12021108c5328de773
```{r libs, include = FALSE}
# Column Water Vapor correction - Modified script for correcting AOD measurements 
# The original script is 07_feature_selection_MOD1a.R, also included in this repo.
library(autoxgboost) # for auto xgboost
library(xgboost)
library(googledrive)
library(data.table)
options(digits = 6)
```

```{r data, include = FALSE}
# Data Preparation using data.table -----------------------------
# https://drive.google.com/open?id=1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN
id <- "1pHE8ktykKfBN2_yo8FNxYX_BFKp1S0SN" # google file ID
mydata <- fread(sprintf("https://docs.google.com/uc?id=%s&export=download", id),
                stringsAsFactors = TRUE)

# clean variables names, remove "/".
names(mydata) <- gsub("/", "", names(mydata)) 

# create variable: Share_Temporary
# The dependent variable to be modeled: Share of temporary structure in slums
mydata[, Share_Temporary := CC7_Structures_Temporary / (CC7_Structures_Temporary + CC6_Structures_Permanent)]

# remove variables with over 20% NA, var_keeps those is.na < 20%
var_keep <- sapply(mydata, function(x) sum(is.na(x))) < dim(mydata)[1]*0.2
var_keep[length(var_keep)] <- TRUE # keep "Share_Temporary"
mydata[, names(mydata)[!var_keep]:= NULL]
mydata <- na.omit(mydata) # remove rest N.A.
# fix some format issue
Factor_List <- c("GG4_Toilets_Pay_Amount", "CC11_Population_Estimate", "CC10_Household_Size",
                 "CC12_Total_Population")
mydata[, (Factor_List) := lapply(.SD, as.numeric), .SDcols = Factor_List]

# remove useless factor
mydata[, c("Obs. Count", "City", "Country",
                       names(mydata)[grepl("Structure", names(mydata))],
                       names(mydata)[grepl("Ownership", names(mydata))]):=NULL]
# remove what are highly correlated to dependent variable
# write.csv(mydata, file = "D:/mydata_slum.csv")

# divide training and test dataset
set.seed(123)
train_idx <- sample(dim(mydata)[1], dim(mydata)[1]* 0.6)

# The model.matrix() function is used in many regression packages for building 
# an "X" matrix from data.
# need matrix for glmnet
X2 <- model.matrix(Share_Temporary~., data = mydata)
Y2 <- as.matrix(mydata[,"Share_Temporary"])
X_train <- X2[train_idx,]
X_test <- X2[-train_idx,]
Y_train <- Y2[train_idx]
Y_test <- Y2[-train_idx]
# merge back to df again, as df is required for regsubsets (best subset)

data_train <- data.frame(X2, Y2)[train_idx,] 
data_test <- data.frame(X2, Y2)[-train_idx,] 
```

# Old Result  
* Parameters  
```{r}
tunegrid <- structure(list(nrounds = 228, max_depth = 8, eta = 0.034, gamma = 0, 
                           colsample_bytree = 0.7208, min_child_weight = 7, 
                           subsample = 0.7017), 
                      Names = c("nrounds","max_depth", "eta", "gamma", 
                                "colsample_bytree", "min_child_weight","subsample"),
                      row.names = 1L, class = "data.frame")
tunegrid
```
* rmse   
```{r, echo = T}
# 0.0433
```

# Using `autoxgboost` 
* A paper on _[Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf)_  
* A presentation: _[Introduction to Bayesian Optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf)_  
* By default, the optimizer runs for for 160 iterations or 1 hour, results using 80 iterations are good enough    
* By default, `par.set`: parameter set to tune over, is `autoxgbparset`:  
```{r echo = T}
autoxgbparset
```
* This dataset is a regression problem, for classification, use `makeClassifTask` instead of `makeRegrTask` in the `makeRegrTask` function. There are more options for different tasks  
* Use all as default, input a _data.frame_, and that's it...   
```{r, echo = T, eval = F}
library(autoxgboost)
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")
set.seed(1234)
system.time(reg_auto <- autoxgboost(reg_task))
# saveRDS(reg_auto, file = "D:/SDIautoxgboost_80.rds")

```
 

# New Result 
```{r}
# library(googledrive)
# # https://drive.google.com/open?id=1_ton-II-XxncnDBB2fNOT3MtDolOAQtT
# temp <- tempfile(fileext = ".rds")
# drive_download(as_id("1_ton-II-XxncnDBB2fNOT3MtDolOAQtT"), path = temp, overwrite = T)
# reg_auto <- readRDS(temp)
reg_auto <- readRDS('D:/liuyanguu/Blogdown/SDIautoxgboost_80.rds')
print(reg_auto)
```
* Testing mse: 0.047 (rmse is:  0.2168) is quite close to the 0.043 from previous post. But it is much faster using only 57 rounds. Notice that the cross-valiation tuning mse is almost the same: 0.044.
```{r}
xgb_pred <- predict(reg_auto, data_test) 
mean((xgb_pred$data$response - xgb_pred$data$truth)^2)
```

# Tuning over Different Boosters
* `autoxgboost` also allows us to tune over the three types of boosters: _gbtree, gblinear and dart_
* The paraset `autoxgbparset.mixed` was predifined by author, but it seems I still need to load it  
* Here is the [question I consulted on github](https://github.com/ja-thomas/autoxgboost/issues/60#issuecomment-428991564)  
```{r, echo=T, eval=F}
reg_task <- makeRegrTask(data = data_train, target = "Share_Temporary")

autoxgbparset.mixed = makeParamSet(
  makeDiscreteParam("booster", values = c("gbtree", "gblinear", "dart")),
  makeDiscreteParam("sample_type", values = c("uniform", "weighted"), requires = quote(booster == "dart")),
  makeDiscreteParam("normalize_type", values = c("tree", "forest"), requires = quote(booster == "dart")),
  makeNumericParam("rate_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
  makeNumericParam("skip_drop", lower = 0, upper = 1, requires = quote(booster == "dart")),
  makeLogicalParam("one_drop", requires = quote(booster == "dart")),
  makeDiscreteParam("grow_policy", values = c("depthwise", "lossguide")),
  makeIntegerParam("max_leaves", lower = 0, upper = 8, trafo = function(x) 2^x, requires = quote(grow_policy == "lossguide")),
  makeIntegerParam("max_bin", lower = 2L, upper = 9, trafo = function(x) 2^x),
  makeNumericParam("eta", lower = 0.01, upper = 0.2),
  makeNumericParam("gamma", lower = -7, upper = 6, trafo = function(x) 2^x),
  makeIntegerParam("max_depth", lower = 3, upper = 20),
  makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
  makeNumericParam("colsample_bylevel", lower = 0.5, upper = 1),
  makeNumericParam("lambda", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam("alpha", lower = -10, upper = 10, trafo = function(x) 2^x),
  makeNumericParam("subsample", lower = 0.5, upper = 1)
)
system.time(reg_auto_dart <- autoxgboost(reg_task, par.set = autoxgbparset.mixed))

```

* Interesting enough, a model with dart booster has been chosen, but the results are pretty much the same. Tuning mse = 0.044, and testing mse = 0.048. It means a result around 0.044 is about the best we can achieve through `xgboost`.  
```{r}
# saveRDS(reg_auto_dart, file = "D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
reg_auto_dart <- readRDS("D:/liuyanguu/Blogdown/SDIautoxgboost_80_dart.rds")
print(reg_auto_dart)
xgb_pred <- predict(reg_auto_dart, data_test) 
cat("testing mse: \n")
mean((xgb_pred$data$response - Y_test)^2)
```

